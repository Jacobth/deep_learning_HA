{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. You should not need to create any new cells in the notebook, but feel free to do it if convenient for you.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may be corrupted if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Fill in your group number and the full names of the members in the cell below;\n",
    "8. Make sure that you are not running an old version of IPython (we provide you with a cell that checks this, make sure you can run it without errors).\n",
    "\n",
    "Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you the following steps before submission for ensuring that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use Google Cloud's GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUP = \"\"\n",
    "NAME1 = \"\"\n",
    "NAME2 = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you can run the following cell without errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "assert IPython.version_info[0] >= 3, \"Your version of IPython is too old, please update it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Assignment 3\n",
    "This home assignment will focus on reinforcement learning and deep reinforcement learning. The first part will cover value-table reinforcement learning techniques, and the second part will include neural networks as function approximators, i.e. deep reinforcement learning. \n",
    "\n",
    "When handing in this assignment, make sure that you're handing in the correct version, and more importantly, *that you do no clear any output from your cells*. We'll use these outputs to aid us when grading your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Gridworld\n",
    "\n",
    "In this task, you will implement Value Iteration to solve for the optimal policy, $\\pi^*$, and the corresponding state value function, $V^*$.\n",
    "\n",
    "The MDP you will work with in this assignment is illustrated in the figure below\n",
    "\n",
    "![title](./grid_world.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent starts in one of the squares shown in the above figure, and then proceeds to take actions. The available actions at any time step are: **North, West, South,** and **East**. If an action would make the agent bump into a wall, or one of the black (unreachable) states, it instead does nothing, leaving the agent at the same place it was before.\n",
    "\n",
    "The reward $R_s^a$ of being in state $s$ and performing actions $a$ is zero for all states, regardless of the action taken, with the exception of the green and the red squares. For the green square, the reward is always 1, and for the red square, always -1, regardless of the action.\n",
    "\n",
    "When the agent is either in the green or the red square, it will be transported to the terminal state in the next time step, regardless of the action taken. The terminal state is shown as the white square with the \"T\" inside.\n",
    "\n",
    "#### State representation\n",
    "The notations used to define the states are illustrated in the table below\n",
    "\n",
    "| $S_0$ | $S_1$ | $S_2$ | $S_3$ | $S_4$ |    |\n",
    "|-------|-------|-------|-------|-------|----|\n",
    "| $S_5$ | $S_6$ | $S_7$ | $S_8$ | $S_9$ |    |\n",
    "| $S_{10}$ | $S_{11}$ | $S_{12}$ | $S_{13}$ | $S_{14}$ | $S_{15}$|\n",
    "\n",
    "where $S_{10}$ corresponds to the initial state of the environment, $S_4$ and $S_9$ to the green and red states of the environment, and $S_{15}$ to the terminal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 1.a: Solve for $V^*(s)$ and $Q^*(s,a)$\n",
    "For this task all transition probabilities are assumed to be 1 (that is, trying to move in a certain direction will definitely move the agent in the chosen direction), and a discount factor of .9, i.e. $\\gamma=.9$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solve for $V^*(S_{10})$ \n",
    "\n",
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solve $Q^*(S_{10},a)$ for all actions\n",
    "\n",
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Task 1.b Write a mathematical expression relating $V^\\pi(s)$ to $Q^\\pi(s,a)$ and $\\pi(a|s)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Task 1.c: Value Iteration\n",
    "For this task, the transitions are no longer deterministic. Instead, there is a 0.2 probability that the agent will try to travel in an orthogonal direction of the chosen action (0.1 probability for each of the two orthogonal directions). Note that the Markov decision process is still known and does not have to be learned from experience.\n",
    "\n",
    "Your task is to implement value iteration and solve for the\n",
    "* optimal greedy policy $\\pi^*(s)$ \n",
    "* $V^*(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The value iteration algorithm\n",
    "Value iteration is an iterative algorithm used to compute the optimal value function $V^*(s)$. Each iteration starts with a guess of what the value function is and then uses the Bellman equations to improve this guess iteratively. We can describe one iteration of the algorithm as\n",
    "\n",
    "$\n",
    "\\textbf{For} \\quad s \\in {\\cal S}:\\qquad  \\\\\n",
    "\\quad \\textbf{For} \\quad \\, a \\in {\\cal A}: \\\\\n",
    "\\qquad Q(s,a) = \\sum_{s'\\in S} T(s,a,s')\\left(R(s,a,s') + \\gamma V(s') \\right)\\\\ \n",
    "\\quad V(s) = \\underset{a}{\\text{max}}~ Q(s,a)\n",
    "$\n",
    "\n",
    "where $T(s, a, s')={\\mathrm Pr}[S'=s'\\big|S=s,A=a]$ is the probability to transition state $s$ to $s'$ given action $a$.\n",
    "\n",
    "\n",
    "#### The MDP Python class\n",
    "The Markov Decision Process you will work with is defined in `gridworld_mpd.py`. In the implementation, the actions are represented by integers as, North = 0, West = 1, South = 2, and East = 3.\n",
    "To interact with the MDP, you need to instantiate an object as: \n",
    "\n",
    "\n",
    "```python\n",
    "mdp = GridWorldMDP()\n",
    "```\n",
    "\n",
    "At your disposal there are a number of instance-functions implemented for you, and presented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_states in module gridworld_mdp:\n",
      "\n",
      "get_states(self)\n",
      "    Returns complete set of states for the MDP\n",
      "    :return: numpy array of shape [num states,]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gridworld_mdp import *\n",
    "import numpy as np\n",
    "\n",
    "help(GridWorldMDP.get_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module gridworld_mdp:\n",
      "\n",
      "__init__(self, trans_prob=0.8)\n",
      "    Initializes an instance of the GridWorldMDP class\n",
      "    :param trans_prob: transition probabilities (e.g. =1 for deterministic MDP)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The constructor\n",
    "help(GridWorldMDP.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_actions in module gridworld_mdp:\n",
      "\n",
      "get_actions(self)\n",
      "    Returns complete set of actions for the MDP\n",
      "    :return: numpy array of shape [num actions,]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.get_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function state_transition_func in module gridworld_mdp:\n",
      "\n",
      "state_transition_func(self, s, a)\n",
      "    Returns the transition probabilities to all states given current state and action\n",
      "    :param state: current state as integer\n",
      "    :param action: selected action as integer\n",
      "    :return: state-transition probabilities, i.e.\n",
      "     [P[S_0| S=s, A_t=a], P[S_1| S=s, A=a], ..., P[S_14| S=s, A=a]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.state_transition_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reward_function in module gridworld_mdp:\n",
      "\n",
      "reward_function(self, s, a)\n",
      "    Returns the reward r(s,a)\n",
      "    :param state: current state as integer\n",
      "    :param action: selected action as integer\n",
      "    :return: r(s,a)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.reward_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide two helper functions for visualizing the value function and the policies you obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for printing a policy pi\n",
    "def print_policy(pi):\n",
    "    print('Policy for non-terminal states: ')\n",
    "    indencies = np.arange(1, 16)\n",
    "    txt = '| '\n",
    "    hor_delimiter = '---------------------'\n",
    "    print(hor_delimiter)\n",
    "    for a, i in zip(pi, indencies):\n",
    "        txt += mdp.act_to_char_dict[a] + ' | '\n",
    "        if i % 5 == 0:\n",
    "            print(txt + '\\n' + hor_delimiter)\n",
    "            txt = '| '\n",
    "    print('                            ---')\n",
    "    print('Policy for terminal state: |', mdp.act_to_char_dict[pi[15]],'|')\n",
    "    print('                            ---')            \n",
    "\n",
    "# Function for printing a table with of the value function\n",
    "def print_value_table(values, num_iterations=None):            \n",
    "    if num_iterations:\n",
    "        print('Values for non-terminal states after: ', num_iterations, 'iterations \\n', np.reshape(values, [3, 5]), '\\n')\n",
    "        print('Value for terminal state:', terminal_value, '\\n')\n",
    "    else: \n",
    "        terminal_value = values[-1]\n",
    "        print('Values for non-terminal states: \\n', np.reshape(values[:-1], [3, 5]))\n",
    "        print('Value for terminal state:', terminal_value, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time for you to implement your own version of value iteration to solve for the greedy policy and $V^*(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(gamma, mdp):\n",
    "    V = np.zeros([16]) # state value table\n",
    "    Q = np.zeros([16, 4]) # state action value table\n",
    "    pi = np.zeros([16]) # greedy policy table\n",
    "\n",
    "    # Complete this function\n",
    "    \n",
    "    S = mdp.get_states()\n",
    "    A = mdp.get_actions()\n",
    "\n",
    "    v_tmp = np.random.random_sample(V.shape)\n",
    "    \n",
    "    while not np.array_equal(v_tmp, V):\n",
    "        \n",
    "        v_tmp = np.copy(V)\n",
    "        \n",
    "        for s in S:    \n",
    "            for a in A:\n",
    "                Q[s,a] = np.sum([np.dot(mdp.state_transition_func(s, a)[s_p], mdp.reward_function(s, a) + gamma*V[s_p]) for s_p in S])\n",
    "\n",
    "            V[s] = np.max(Q[s])    \n",
    "            pi[s] = np.argmax(Q[s])\n",
    "        \n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your implementation for the deterministic version of our MDP. As a sanity check, compare your analytical solutions with the output from your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for non-terminal states: \n",
      " [[ 0.6561    0.729     0.81      0.9       1.      ]\n",
      " [ 0.59049   0.        0.        0.81     -1.      ]\n",
      " [ 0.531441  0.59049   0.6561    0.729     0.6561  ]]\n",
      "Value for terminal state: 0.0 \n",
      "\n",
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | N | \n",
      "---------------------\n",
      "| N | N | N | N | N | \n",
      "---------------------\n",
      "| N | E | E | N | W | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "mdp = GridWorldMDP(trans_prob=1.)\n",
    "v, pi = value_iteration(.9, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once your implementation passed the sanity check, run it for the stochastic case, where the probability of an action succeding is 0.8, and 0.2 of moving the agent in an orthogonal direction to the intended. Use $\\gamma = .99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for non-terminal states: \n",
      " [[ 0.93861973  0.95193393  0.9639533   0.97612443  1.        ]\n",
      " [ 0.92691625  0.          0.          0.88371826 -1.        ]\n",
      " [ 0.91395196  0.90255605  0.89130223  0.88057656  0.79978972]]\n",
      "Value for terminal state: 0.0 \n",
      "\n",
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | N | \n",
      "---------------------\n",
      "| N | N | N | W | N | \n",
      "---------------------\n",
      "| N | W | W | W | S | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "# Run for stochastic MDP, gamma = .99\n",
    "mdp = GridWorldMDP()\n",
    "v, pi = value_iteration(.99, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the policy that the algorithm found looks reasonable? For instance, what's the policy for state $S_8$? Is that a good idea? Why?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: state-value test, for gamma=.99\n",
      "Passed: policy test, for gamma=.99\n"
     ]
    }
   ],
   "source": [
    "test_value_iteration(v, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run value iteration for the same scenario as above, but now with $\\gamma=.9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for non-terminal states: \n",
      " [[ 0.56631445  0.65360208  0.74438015  0.84776628  1.        ]\n",
      " [ 0.49725171  0.          0.          0.57185903 -1.        ]\n",
      " [ 0.43084446  0.37830245  0.41624465  0.47405641  0.2761765 ]]\n",
      "Value for terminal state: 0.0 \n",
      "\n",
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | N | \n",
      "---------------------\n",
      "| N | N | N | N | N | \n",
      "---------------------\n",
      "| N | W | E | N | W | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "# Run for stochastic MDP, gamma = .9\n",
    "mdp = GridWorldMDP()\n",
    "v, pi = value_iteration(.9, mdp)\n",
    "print_value_table(v)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice any difference between the greedy policy for the two different discount factors. If so, what's the difference, and why do you think this happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Q-learning\n",
    "\n",
    "In the previous task, you solved for $V^*(s)$ and the greedy policy $\\pi^*(s)$, with the entire model of the MDP being available to you. This is however not very practical since for most problems we are trying to solve, the model is not known, and estimating the model is quite often a very tedious process which often also requires a lot of simplifications. \n",
    "\n",
    "#### Q-learning algorithm\n",
    "$\n",
    "\\text{Initialize}~Q(s,a), ~ \\forall~ s \\in {\\cal S},~ a~\\in {\\cal A} \\\\\n",
    "\\textbf{Repeat}~\\text{(for each episode):}\\\\\n",
    "\\quad \\text{Initialize}~s\\\\\n",
    "\\qquad \\textbf{Repeat}~\\text{(for each step in episode):}\\\\\n",
    "\\qquad\\quad \\text{Chose $a$ from $s$ using poliy derived from $Q$ (e.g., $\\epsilon$-greedy)}\\\\\n",
    "\\qquad\\quad \\text{Take action a, observe r, s'}\\\\\n",
    "\\qquad\\quad Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left(r + \\gamma~\\underset{a}{\\text{max}}~Q(s',a) - Q(s,a) \\right) \\\\\n",
    "\\qquad\\quad s \\leftarrow s' \\\\\n",
    "\\qquad \\text{Until s is terminal}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 Model-free control\n",
    "Why is it that Q-learning does not require a model of the MDP to solve for it?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2  Implement an $\\epsilon$-greedy policy\n",
    "The goal of the Q-learning algorithm is to find the optimal policy $\\pi^*$, by estimating the state action value function under the optimal policy, i.e. $Q^*(s, a)$. From $Q^*(s,a)$, the agent can follow $\\pi^*$, by choosing the action with that yields the largest expected value for each state, i.e. $\\underset{a}{\\text{argmax}}~Q^*(s, a)$.\n",
    "\n",
    "However, when training a Q-learning model, the agent typically follows another policy to explore the environment. In reinforcement learning this is known as off-policy learning. \n",
    "\n",
    "Your task is to implement a widely popular exploration policy, known as  the $\\epsilon$-greedy policy, in the cell below.\n",
    "\n",
    "An $\\epsilon$-Greedy policy should:\n",
    "* with probability $\\epsilon$ take an uniformly-random action.\n",
    "* otherwise choose the best action according to the estimated state action values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def eps_greedy_policy(q_values, eps):\n",
    "    '''\n",
    "    Creates an epsilon-greedy policy\n",
    "    :param q_values: set of Q-values of shape (num actions,)\n",
    "    :param eps: probability of taking a uniform random action \n",
    "    :return: policy of shape (num actions,)\n",
    "    '''\n",
    "    # Complete this function\n",
    "    policy = np.zeros(q_values.shape)\n",
    "    rand = random.uniform(0, 1)\n",
    "    \n",
    "    if rand < eps:\n",
    "        policy[:] = 1 / len(policy)\n",
    "    \n",
    "    else:\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[best_action] = 1\n",
    "        \n",
    "    return policy    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed, good job!\n"
     ]
    }
   ],
   "source": [
    "mdp = GridWorldMDP()\n",
    "\n",
    "# Test shape of output\n",
    "actions = mdp.get_actions()\n",
    "for eps in (0, 1):\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[0] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, eps)\n",
    "    assert foo.shape == eps_greedy.shape, \"wrong shape of output\"\n",
    "actions = [i for i in range(10)]\n",
    "for eps in (0, 1):\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[0] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, eps)\n",
    "    assert foo.shape == eps_greedy.shape, \"wrong shape of output\"\n",
    "\n",
    "# Test for greedy actions\n",
    "for a in actions:\n",
    "    foo = np.zeros([len(actions)])\n",
    "    foo[a] = 1.\n",
    "    eps_greedy = eps_greedy_policy(foo, 0)\n",
    "    assert np.array_equal(foo, eps_greedy), \"policy is not greedy\"\n",
    "\n",
    "# Test for uniform distribution, when eps=1\n",
    "eps_greedy = eps_greedy_policy(foo, 1)\n",
    "assert all(p==eps_greedy[0] for p in eps_greedy) and np.sum(eps_greedy)==1, \\\n",
    "\"policy does not return a uniform distribution for eps=1\"\n",
    "\n",
    "print('Test passed, good job!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Implement the Q-learning algorithm\n",
    "\n",
    "Now it's time to actually implement the Q-learning algorithm. Unlike the Value iteration where there is no direct interactions with the environment, the Q-learning algorithm builds up its estimations by interacting and exploring the environment. \n",
    "\n",
    "To enable the agent to explore the environment a set of helper functions are provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reset in module gridworld_mdp:\n",
      "\n",
      "reset(self)\n",
      "    Resets the environment and the agent is positioned in the initial state in the bottom left corner.\n",
      "    :return: state, reward, terminal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function step in module gridworld_mdp:\n",
      "\n",
      "step(self, action)\n",
      "    Takes one step in the environment using the selected action\n",
      "    :param action: action to execute, integer\n",
      "    :return: state, reward, terminal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(GridWorldMDP.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your version of Q-learning in the cell below. \n",
    "\n",
    "**Hint:** It might be useful to study the pseudocode provided above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(eps, gamma):\n",
    "    Q = np.zeros([16, 4]) # state action value table\n",
    "    pi = np.zeros([16]) # greedy policy table\n",
    "    alpha = .01\n",
    "    \n",
    "    # Complete this function\n",
    "    episodes = 5000\n",
    "    mdp = GridWorldMDP()\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        \n",
    "        state, reward, terminal = mdp.reset()\n",
    "        \n",
    "        while(not terminal):\n",
    "            \n",
    "            actions = eps_greedy_policy(Q[state], eps)\n",
    "            action = np.argmax(actions)\n",
    "            \n",
    "            if action != 1:\n",
    "                action = random.randint(0, len(actions) - 1)\n",
    "            \n",
    "            state_p, reward, terminal = mdp.step(action)\n",
    "    \n",
    "            Q[state,action] = Q[state,action] + alpha*(reward + (gamma * np.max(Q[state_p]) - Q[state,action]))\n",
    "            \n",
    "            state = state_p\n",
    "            \n",
    "    pi = np.argmax(Q, axis=1)\n",
    "    \n",
    "    return pi, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q-learning with  $\\epsilon = 1$ for the MDP with $\\gamma=0.99$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | W | \n",
      "---------------------\n",
      "| N | N | N | N | N | \n",
      "---------------------\n",
      "| N | W | W | W | W | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "pi, Q = q_learning(1, .99)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed: policy test, for gamma=.99\n"
     ]
    }
   ],
   "source": [
    "test_q_learning(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Q-learning with $\\epsilon=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for non-terminal states: \n",
      "---------------------\n",
      "| E | E | E | E | W | \n",
      "---------------------\n",
      "| N | N | N | W | N | \n",
      "---------------------\n",
      "| N | W | W | W | N | \n",
      "---------------------\n",
      "                            ---\n",
      "Policy for terminal state: | N |\n",
      "                            ---\n"
     ]
    }
   ],
   "source": [
    "pi, Q = q_learning(0, .99)\n",
    "print_policy(pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You ran your implementation with $\\epsilon$ set to both 0 and 1. What are the results, and your conclusions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Deep Double Q-learning (DDQN)\n",
    "For this task, you will implement a DDQN (double deep Q-learning network) to solve one of the problems of the OpenAI gym. Before we get into details about these type of networks, let's first review the simpler, DQN (deep Q-learning network) version. \n",
    "\n",
    "#### Deep Q Networks\n",
    "As we saw in the video lectures, using a neural network as a state action value approximator is a great idea. However, if one tries to use this approach with Q-learning, it's very likely that the optimization will be very unstable. To remediate this, two main ideas are used. First, we use experience replay, in order to decorrelate the experience samples we obtain when exploring the environment. Second, we use two networks instead of one, in order to fix the optimization targets. That is, for a given minibatch sampled from the replay buffer, we'll optimize the weights of only one of the networks (commonly denoted as the \"online\" network), using the gradients w.r.t a loss function. This loss function is computed as the mean squared error between the current action values, computed according to the **online** network, and the temporal difference (TD) targets, computed using the other, **fixed network** (which we'll refer to as the \"target\" network).\n",
    "\n",
    "That is, the loss function is \n",
    "\n",
    "$$ L(\\theta) = \\frac{1}{N}\\sum_{i=1}^N \\left(Q(s_i,a_i; \\theta\\right) - Y_i)^2~,$$\n",
    "\n",
    "where $N$ is the number of samples in your minibatch, $Q(s,a;\\theta)$ is the state action value estimate, according to the online network (with parameters $\\theta$), and $Y_t$ is the TD target, computed as\n",
    "\n",
    "$$ Y_i = r_i +  \\gamma ~\\underset{a}{\\text{max}}~Q(s_i', a; \\theta^-)~, $$\n",
    "\n",
    "where $Q(s', a;\\theta')$ is the action value estimate, according to the fixed network (with parameters $\\theta^-$).\n",
    "\n",
    "Finally, so that the offline parameters are also updated, we periodically change the roles of the networks, fixing the online one, and training the other.\n",
    "\n",
    "#### Double Deep Q Networks\n",
    "\n",
    "The idea explained above works well in practice, but later it was discovered that this approach is very prone to overestimating the state action values. The main reason for this is that the max operator, used to select the greedy action when computing the TD target, uses the same values both to select and to evaluate an action (this tends to prefer overestimated actions). In order to prevent this, we can decouple the selection from the evaluation, which is the idea that created DDQN. More concretely, the TD target for a DDQN is now \n",
    "\n",
    "$$ Y_i = r_i + \\gamma Q(s_i', \\underset{a}{\\text{argmax}}Q(s_i',a;\\theta); \\theta^-)~. $$\n",
    "\n",
    "Hence, we're using the **online** network to select which action is best, but we use the **fixed** network to evaluate the state action value for that chosen action in the next state. This is what makes DDQN not overestimate (as much) the state action values, which in turn helps us to train faster and obtain better policies.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "\n",
    "The problem you will solve for this task is the inverted pendulum problem. \n",
    "On [Open AIs environment documentation](https://gym.openai.com/envs/CartPole-v0) , the following description is provided:\n",
    "\n",
    "*A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every time step that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.*\n",
    "\n",
    "![title](./cartpole.jpg) \n",
    "\n",
    "#### Implementation\n",
    "We'll solve this task using a DDQN. Most of the code is provided for you, in the file **ddqn_model.py**. This file contains the implementation of a neural network, which is described in the table below (feel free to experiment with different architectures).\n",
    "\n",
    "|Layer 1: units, activation | Layer 2: units, activation | Layer 3: units, activation | Cost function |\n",
    "|---------------------------|----------------------------|----------------------------|---------------|\n",
    "| 100, ReLu                 | 60, ReLu                   | number of actions, linear | MSE           |\n",
    "\n",
    "The only missing part of the code is the function that computes the TD targets for each minibatch of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1:  Calculate TD-target\n",
    "\n",
    "For this task, you will calculate the temporal difference target used for the loss in the double Q-learning algorithm. Your implementation should follow precisely the equation defined above for the TD target of DDQNs, with one exception: when s' is terminal, the TD target for it should simply be $ Y_i = r_i$. Why is this necessary?\n",
    "\n",
    "**Your answer**: (fill in here)\n",
    "\n",
    "Implement your function in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_td_targets(q1_batch, q2_batch, r_batch, t_batch, gamma=.99):\n",
    "    '''\n",
    "    Calculates the TD-target used for the loss\n",
    "    : param q1_batch: Batch of Q(s', a) from online network, shape (N, num actions)\n",
    "    : param q2_batch: Batch of Q(s', a) from target network, shape (N, num actions)\n",
    "    : param r_batch: Batch of rewards, shape (N, 1)\n",
    "    : param t_batch: Batch of booleans indicating if state, s' is terminal, shape (N, 1)\n",
    "    : return: TD-target, shape (N, 1)\n",
    "    '''\n",
    "    \n",
    "    # Complete this function\n",
    "    Y = np.zeros(r_batch.shape)\n",
    "    N = len(r_batch)\n",
    "    \n",
    "    Y = np.zeros(r_batch.shape)\n",
    "    \n",
    "    for i in range(N):\n",
    "        if t_batch[i]:\n",
    "            Y[i] = r_batch[i]\n",
    "        else:\n",
    "            Y[i] = r_batch[i] + gamma * q2_batch[i, np.argmax(q1_batch[i])]\n",
    "    \n",
    "    #Y = np.array([r_bacth[i] + gamma * (q1_batch[i, np.argmax(q2_batch[i])]) for i in range(N) if x % 2 == 0])\n",
    "    \n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your implementation by trying to solve the reinforcement learning problem for the Cartpole environment. The following cell defines the `train_loop_ddqn` function, which will be called ahead,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import gym\n",
    "from keras.utils.np_utils import to_categorical as one_hot\n",
    "from collections import namedtuple\n",
    "from dqn_model import DoubleQLearningModel, ExperienceReplay\n",
    "\n",
    "def train_loop_ddqn(model, env, num_episodes, batch_size=64, gamma=.94):        \n",
    "    Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"next_s\", \"t\"])\n",
    "    eps = 1.\n",
    "    eps_end = .1 \n",
    "    eps_decay = .001\n",
    "    R_buffer = []\n",
    "    R_avg = []\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset() #reset to initial state\n",
    "        state = np.expand_dims(state, axis=0)/2\n",
    "        terminal = False # reset terminal flag\n",
    "        ep_reward = 0\n",
    "        q_buffer = []\n",
    "        steps = 0\n",
    "        while not terminal:\n",
    "            #env.render() # comment this line out if ou don't want to render the environment\n",
    "            steps += 1\n",
    "            q_values = model.get_q_values(state)\n",
    "            q_buffer.append(q_values)\n",
    "            policy = eps_greedy_policy(q_values.squeeze(), eps) \n",
    "            action = np.random.choice(num_actions, p=policy) # sample action from epsilon-greedy policy\n",
    "            new_state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "            new_state = np.expand_dims(new_state, axis=0)/2\n",
    "            \n",
    "            # only use the terminal flag for ending the episode and not for training\n",
    "            # if the flag is set due to that the maximum amount of steps is reached \n",
    "            t_to_buffer = terminal if not steps == 200 else False\n",
    "            \n",
    "            # store data to replay buffer\n",
    "            replay_buffer.add(Transition(s=state, a=action, r=reward, next_s=new_state, t=t_to_buffer))\n",
    "            state = new_state\n",
    "            ep_reward += reward\n",
    "            \n",
    "            # if buffer contains more than 1000 samples, perform one training step\n",
    "            if replay_buffer.buffer_length > 1000:\n",
    "                s, a, r, s_, t = replay_buffer.sample_minibatch(batch_size) # sample a minibatch of transitions\n",
    "                q_1, q_2 = model.get_q_values_for_both_models(np.squeeze(s_))\n",
    "                td_target = calculate_td_targets(q_1, q_2, r, t, gamma)\n",
    "                model.update(s, td_target, a)    \n",
    "                \n",
    "        eps = max(eps - eps_decay, eps_end) # decrease epsilon        \n",
    "        R_buffer.append(ep_reward)\n",
    "        \n",
    "        # running average of episodic rewards\n",
    "        R_avg.append(.05 * R_buffer[i] + .95 * R_avg[i-1]) if i > 0 else R_avg.append(R_buffer[i])\n",
    "        print('Episode: ', i, 'Reward:', ep_reward, 'Epsilon', eps, 'mean q', np.mean(np.array(q_buffer)))\n",
    "        \n",
    "        # if running average > 195, the task is considerd solved\n",
    "        if R_avg[-1] > 195:\n",
    "            return R_buffer, R_avg\n",
    "    return R_buffer, R_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the next cell performs the actual training. \n",
    "\n",
    "A Working implementation should start to improve after 500 episodes. An episodic reward of around 200 is likely to be achieved after 800 episodes for a batchsize of 128, and 1000 episodes for a batchsize of 64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0 Reward: 14.0 Epsilon 0.999 mean q 5.7215157e-08\n",
      "Episode:  1 Reward: 29.0 Epsilon 0.998 mean q 3.9121293e-08\n",
      "Episode:  2 Reward: 19.0 Epsilon 0.997 mean q -9.4691694e-09\n",
      "Episode:  3 Reward: 62.0 Epsilon 0.996 mean q -2.907776e-10\n",
      "Episode:  4 Reward: 12.0 Epsilon 0.995 mean q -1.9930427e-08\n",
      "Episode:  5 Reward: 25.0 Epsilon 0.994 mean q -6.470939e-09\n",
      "Episode:  6 Reward: 11.0 Epsilon 0.993 mean q -1.6986279e-08\n",
      "Episode:  7 Reward: 13.0 Epsilon 0.992 mean q 5.2236366e-08\n",
      "Episode:  8 Reward: 9.0 Epsilon 0.991 mean q -1.19094254e-08\n",
      "Episode:  9 Reward: 40.0 Epsilon 0.99 mean q -3.3745255e-08\n",
      "Episode:  10 Reward: 14.0 Epsilon 0.989 mean q 1.525917e-08\n",
      "Episode:  11 Reward: 15.0 Epsilon 0.988 mean q 2.5052737e-08\n",
      "Episode:  12 Reward: 11.0 Epsilon 0.987 mean q -3.6750667e-08\n",
      "Episode:  13 Reward: 17.0 Epsilon 0.986 mean q 5.8163703e-08\n",
      "Episode:  14 Reward: 26.0 Epsilon 0.985 mean q 1.3073452e-08\n",
      "Episode:  15 Reward: 27.0 Epsilon 0.984 mean q 3.960525e-08\n",
      "Episode:  16 Reward: 16.0 Epsilon 0.983 mean q -3.3878422e-08\n",
      "Episode:  17 Reward: 12.0 Epsilon 0.982 mean q 4.3519943e-08\n",
      "Episode:  18 Reward: 31.0 Epsilon 0.981 mean q 4.890599e-09\n",
      "Episode:  19 Reward: 41.0 Epsilon 0.98 mean q 7.397522e-08\n",
      "Episode:  20 Reward: 12.0 Epsilon 0.979 mean q 5.8330745e-08\n",
      "Episode:  21 Reward: 24.0 Epsilon 0.978 mean q 3.6898214e-08\n",
      "Episode:  22 Reward: 29.0 Epsilon 0.977 mean q 3.3131354e-08\n",
      "Episode:  23 Reward: 26.0 Epsilon 0.976 mean q 4.9565734e-09\n",
      "Episode:  24 Reward: 38.0 Epsilon 0.975 mean q 9.133502e-09\n",
      "Episode:  25 Reward: 10.0 Epsilon 0.974 mean q -5.0767774e-08\n",
      "Episode:  26 Reward: 28.0 Epsilon 0.973 mean q -3.2354155e-08\n",
      "Episode:  27 Reward: 13.0 Epsilon 0.972 mean q 3.8671505e-08\n",
      "Episode:  28 Reward: 15.0 Epsilon 0.971 mean q -2.8449147e-09\n",
      "Episode:  29 Reward: 16.0 Epsilon 0.97 mean q -1.9152807e-08\n",
      "Episode:  30 Reward: 13.0 Epsilon 0.969 mean q 3.6296214e-08\n",
      "Episode:  31 Reward: 30.0 Epsilon 0.968 mean q -4.558795e-09\n",
      "Episode:  32 Reward: 10.0 Epsilon 0.967 mean q 6.38901e-08\n",
      "Episode:  33 Reward: 42.0 Epsilon 0.966 mean q 1.5136331e-08\n",
      "Episode:  34 Reward: 16.0 Epsilon 0.965 mean q -2.9666579e-08\n",
      "Episode:  35 Reward: 24.0 Epsilon 0.964 mean q -2.1246592e-08\n",
      "Episode:  36 Reward: 17.0 Epsilon 0.963 mean q -2.4202203e-08\n",
      "Episode:  37 Reward: 12.0 Epsilon 0.962 mean q -1.5383263e-08\n",
      "Episode:  38 Reward: 12.0 Epsilon 0.961 mean q 9.790339e-08\n",
      "Episode:  39 Reward: 41.0 Epsilon 0.96 mean q 5.286662e-08\n",
      "Episode:  40 Reward: 18.0 Epsilon 0.959 mean q -4.0585387e-08\n",
      "Episode:  41 Reward: 14.0 Epsilon 0.958 mean q 1.3655601e-08\n",
      "Episode:  42 Reward: 11.0 Epsilon 0.957 mean q 9.441258e-08\n",
      "Episode:  43 Reward: 18.0 Epsilon 0.956 mean q 2.0702515e-08\n",
      "Episode:  44 Reward: 14.0 Epsilon 0.955 mean q 8.63647e-08\n",
      "Episode:  45 Reward: 26.0 Epsilon 0.954 mean q 5.137857e-09\n",
      "Episode:  46 Reward: 25.0 Epsilon 0.953 mean q 4.5636543e-09\n",
      "Episode:  47 Reward: 34.0 Epsilon 0.952 mean q 0.0033396364\n",
      "Episode:  48 Reward: 23.0 Epsilon 0.951 mean q 0.0111625455\n",
      "Episode:  49 Reward: 16.0 Epsilon 0.95 mean q 0.01778759\n",
      "Episode:  50 Reward: 22.0 Epsilon 0.949 mean q 0.026768874\n",
      "Episode:  51 Reward: 10.0 Epsilon 0.948 mean q 0.04571862\n",
      "Episode:  52 Reward: 11.0 Epsilon 0.947 mean q 0.061866578\n",
      "Episode:  53 Reward: 63.0 Epsilon 0.946 mean q 0.067736804\n",
      "Episode:  54 Reward: 24.0 Epsilon 0.945 mean q 0.1155634\n",
      "Episode:  55 Reward: 23.0 Epsilon 0.944 mean q 0.104107566\n",
      "Episode:  56 Reward: 15.0 Epsilon 0.943 mean q 0.11319907\n",
      "Episode:  57 Reward: 12.0 Epsilon 0.942 mean q 0.15058574\n",
      "Episode:  58 Reward: 29.0 Epsilon 0.941 mean q 0.12696353\n",
      "Episode:  59 Reward: 16.0 Epsilon 0.94 mean q 0.19734143\n",
      "Episode:  60 Reward: 28.0 Epsilon 0.939 mean q 0.15302193\n",
      "Episode:  61 Reward: 9.0 Epsilon 0.938 mean q 0.31050232\n",
      "Episode:  62 Reward: 28.0 Epsilon 0.9369999999999999 mean q 0.1795155\n",
      "Episode:  63 Reward: 24.0 Epsilon 0.9359999999999999 mean q 0.2074821\n",
      "Episode:  64 Reward: 21.0 Epsilon 0.9349999999999999 mean q 0.2620829\n",
      "Episode:  65 Reward: 27.0 Epsilon 0.9339999999999999 mean q 0.2658728\n",
      "Episode:  66 Reward: 45.0 Epsilon 0.9329999999999999 mean q 0.47211087\n",
      "Episode:  67 Reward: 17.0 Epsilon 0.9319999999999999 mean q 0.45008585\n",
      "Episode:  68 Reward: 13.0 Epsilon 0.9309999999999999 mean q 0.44604254\n",
      "Episode:  69 Reward: 11.0 Epsilon 0.9299999999999999 mean q 0.49725434\n",
      "Episode:  70 Reward: 19.0 Epsilon 0.9289999999999999 mean q 0.48234603\n",
      "Episode:  71 Reward: 15.0 Epsilon 0.9279999999999999 mean q 0.7368006\n",
      "Episode:  72 Reward: 23.0 Epsilon 0.9269999999999999 mean q 0.563849\n",
      "Episode:  73 Reward: 31.0 Epsilon 0.9259999999999999 mean q 0.5667388\n",
      "Episode:  74 Reward: 9.0 Epsilon 0.9249999999999999 mean q 1.084168\n",
      "Episode:  75 Reward: 44.0 Epsilon 0.9239999999999999 mean q 0.57393897\n",
      "Episode:  76 Reward: 12.0 Epsilon 0.9229999999999999 mean q 0.88914776\n",
      "Episode:  77 Reward: 17.0 Epsilon 0.9219999999999999 mean q 0.7608139\n",
      "Episode:  78 Reward: 49.0 Epsilon 0.9209999999999999 mean q 0.85169226\n",
      "Episode:  79 Reward: 13.0 Epsilon 0.9199999999999999 mean q 1.0797362\n",
      "Episode:  80 Reward: 13.0 Epsilon 0.9189999999999999 mean q 1.2621133\n",
      "Episode:  81 Reward: 23.0 Epsilon 0.9179999999999999 mean q 1.0384605\n",
      "Episode:  82 Reward: 38.0 Epsilon 0.9169999999999999 mean q 1.5949627\n",
      "Episode:  83 Reward: 10.0 Epsilon 0.9159999999999999 mean q 2.2968535\n",
      "Episode:  84 Reward: 17.0 Epsilon 0.9149999999999999 mean q 1.5598366\n",
      "Episode:  85 Reward: 19.0 Epsilon 0.9139999999999999 mean q 1.383247\n",
      "Episode:  86 Reward: 14.0 Epsilon 0.9129999999999999 mean q 1.7911888\n",
      "Episode:  87 Reward: 13.0 Epsilon 0.9119999999999999 mean q 1.6701238\n",
      "Episode:  88 Reward: 21.0 Epsilon 0.9109999999999999 mean q 1.8100221\n",
      "Episode:  89 Reward: 16.0 Epsilon 0.9099999999999999 mean q 1.4143162\n",
      "Episode:  90 Reward: 14.0 Epsilon 0.9089999999999999 mean q 1.6326741\n",
      "Episode:  91 Reward: 15.0 Epsilon 0.9079999999999999 mean q 2.1255033\n",
      "Episode:  92 Reward: 18.0 Epsilon 0.9069999999999999 mean q 1.7143738\n",
      "Episode:  93 Reward: 21.0 Epsilon 0.9059999999999999 mean q 1.6747586\n",
      "Episode:  94 Reward: 10.0 Epsilon 0.9049999999999999 mean q 2.2239115\n",
      "Episode:  95 Reward: 40.0 Epsilon 0.9039999999999999 mean q 1.5875767\n",
      "Episode:  96 Reward: 17.0 Epsilon 0.9029999999999999 mean q 1.754516\n",
      "Episode:  97 Reward: 23.0 Epsilon 0.9019999999999999 mean q 2.1915276\n",
      "Episode:  98 Reward: 21.0 Epsilon 0.9009999999999999 mean q 1.9649699\n",
      "Episode:  99 Reward: 16.0 Epsilon 0.8999999999999999 mean q 2.5224822\n",
      "Episode:  100 Reward: 25.0 Epsilon 0.8989999999999999 mean q 1.8921589\n",
      "Episode:  101 Reward: 67.0 Epsilon 0.8979999999999999 mean q 3.1629128\n",
      "Episode:  102 Reward: 14.0 Epsilon 0.8969999999999999 mean q 2.8626235\n",
      "Episode:  103 Reward: 14.0 Epsilon 0.8959999999999999 mean q 2.9699383\n",
      "Episode:  104 Reward: 38.0 Epsilon 0.8949999999999999 mean q 2.6393867\n",
      "Episode:  105 Reward: 20.0 Epsilon 0.8939999999999999 mean q 3.0632472\n",
      "Episode:  106 Reward: 16.0 Epsilon 0.8929999999999999 mean q 3.6478662\n",
      "Episode:  107 Reward: 31.0 Epsilon 0.8919999999999999 mean q 2.7456696\n",
      "Episode:  108 Reward: 43.0 Epsilon 0.8909999999999999 mean q 3.5436437\n",
      "Episode:  109 Reward: 45.0 Epsilon 0.8899999999999999 mean q 3.3849292\n",
      "Episode:  110 Reward: 19.0 Epsilon 0.8889999999999999 mean q 4.4658165\n",
      "Episode:  111 Reward: 10.0 Epsilon 0.8879999999999999 mean q 5.1096053\n",
      "Episode:  112 Reward: 11.0 Epsilon 0.8869999999999999 mean q 5.466164\n",
      "Episode:  113 Reward: 13.0 Epsilon 0.8859999999999999 mean q 5.0858235\n",
      "Episode:  114 Reward: 14.0 Epsilon 0.8849999999999999 mean q 5.8069634\n",
      "Episode:  115 Reward: 13.0 Epsilon 0.8839999999999999 mean q 5.402115\n",
      "Episode:  116 Reward: 15.0 Epsilon 0.8829999999999999 mean q 4.537439\n",
      "Episode:  117 Reward: 20.0 Epsilon 0.8819999999999999 mean q 4.3897147\n",
      "Episode:  118 Reward: 16.0 Epsilon 0.8809999999999999 mean q 5.075829\n",
      "Episode:  119 Reward: 19.0 Epsilon 0.8799999999999999 mean q 4.5957932\n",
      "Episode:  120 Reward: 10.0 Epsilon 0.8789999999999999 mean q 6.4640493\n",
      "Episode:  121 Reward: 13.0 Epsilon 0.8779999999999999 mean q 4.68063\n",
      "Episode:  122 Reward: 21.0 Epsilon 0.8769999999999999 mean q 4.5852547\n",
      "Episode:  123 Reward: 10.0 Epsilon 0.8759999999999999 mean q 5.544836\n",
      "Episode:  124 Reward: 13.0 Epsilon 0.8749999999999999 mean q 4.8261333\n",
      "Episode:  125 Reward: 27.0 Epsilon 0.8739999999999999 mean q 3.8766546\n",
      "Episode:  126 Reward: 12.0 Epsilon 0.8729999999999999 mean q 6.3261986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  127 Reward: 35.0 Epsilon 0.8719999999999999 mean q 4.673741\n",
      "Episode:  128 Reward: 21.0 Epsilon 0.8709999999999999 mean q 4.5072713\n",
      "Episode:  129 Reward: 16.0 Epsilon 0.8699999999999999 mean q 5.0868554\n",
      "Episode:  130 Reward: 36.0 Epsilon 0.8689999999999999 mean q 6.3905897\n",
      "Episode:  131 Reward: 12.0 Epsilon 0.8679999999999999 mean q 6.726131\n",
      "Episode:  132 Reward: 25.0 Epsilon 0.8669999999999999 mean q 4.973524\n",
      "Episode:  133 Reward: 12.0 Epsilon 0.8659999999999999 mean q 6.818747\n",
      "Episode:  134 Reward: 10.0 Epsilon 0.8649999999999999 mean q 8.129108\n",
      "Episode:  135 Reward: 9.0 Epsilon 0.8639999999999999 mean q 7.0768185\n",
      "Episode:  136 Reward: 61.0 Epsilon 0.8629999999999999 mean q 5.683403\n",
      "Episode:  137 Reward: 25.0 Epsilon 0.8619999999999999 mean q 6.2492504\n",
      "Episode:  138 Reward: 12.0 Epsilon 0.8609999999999999 mean q 7.374666\n",
      "Episode:  139 Reward: 14.0 Epsilon 0.8599999999999999 mean q 6.2246523\n",
      "Episode:  140 Reward: 9.0 Epsilon 0.8589999999999999 mean q 8.666249\n",
      "Episode:  141 Reward: 21.0 Epsilon 0.8579999999999999 mean q 6.717631\n",
      "Episode:  142 Reward: 14.0 Epsilon 0.8569999999999999 mean q 7.4799256\n",
      "Episode:  143 Reward: 13.0 Epsilon 0.8559999999999999 mean q 6.381932\n",
      "Episode:  144 Reward: 21.0 Epsilon 0.8549999999999999 mean q 7.166014\n",
      "Episode:  145 Reward: 24.0 Epsilon 0.8539999999999999 mean q 5.860604\n",
      "Episode:  146 Reward: 21.0 Epsilon 0.8529999999999999 mean q 7.606408\n",
      "Episode:  147 Reward: 29.0 Epsilon 0.8519999999999999 mean q 5.6779075\n",
      "Episode:  148 Reward: 26.0 Epsilon 0.8509999999999999 mean q 7.8483396\n",
      "Episode:  149 Reward: 24.0 Epsilon 0.8499999999999999 mean q 6.300865\n",
      "Episode:  150 Reward: 22.0 Epsilon 0.8489999999999999 mean q 6.7995267\n",
      "Episode:  151 Reward: 18.0 Epsilon 0.8479999999999999 mean q 8.093205\n",
      "Episode:  152 Reward: 9.0 Epsilon 0.8469999999999999 mean q 9.897662\n",
      "Episode:  153 Reward: 12.0 Epsilon 0.8459999999999999 mean q 8.627965\n",
      "Episode:  154 Reward: 15.0 Epsilon 0.8449999999999999 mean q 7.8954425\n",
      "Episode:  155 Reward: 26.0 Epsilon 0.8439999999999999 mean q 7.2776923\n",
      "Episode:  156 Reward: 26.0 Epsilon 0.8429999999999999 mean q 6.680127\n",
      "Episode:  157 Reward: 29.0 Epsilon 0.8419999999999999 mean q 7.238489\n",
      "Episode:  158 Reward: 16.0 Epsilon 0.8409999999999999 mean q 8.47096\n",
      "Episode:  159 Reward: 12.0 Epsilon 0.8399999999999999 mean q 10.773839\n",
      "Episode:  160 Reward: 18.0 Epsilon 0.8389999999999999 mean q 7.5522027\n",
      "Episode:  161 Reward: 10.0 Epsilon 0.8379999999999999 mean q 9.003207\n",
      "Episode:  162 Reward: 25.0 Epsilon 0.8369999999999999 mean q 7.777799\n",
      "Episode:  163 Reward: 17.0 Epsilon 0.8359999999999999 mean q 7.2839417\n",
      "Episode:  164 Reward: 17.0 Epsilon 0.8349999999999999 mean q 9.044902\n",
      "Episode:  165 Reward: 10.0 Epsilon 0.8339999999999999 mean q 9.902203\n",
      "Episode:  166 Reward: 23.0 Epsilon 0.8329999999999999 mean q 8.085118\n",
      "Episode:  167 Reward: 15.0 Epsilon 0.8319999999999999 mean q 9.261246\n",
      "Episode:  168 Reward: 11.0 Epsilon 0.8309999999999998 mean q 11.235106\n",
      "Episode:  169 Reward: 11.0 Epsilon 0.8299999999999998 mean q 9.702775\n",
      "Episode:  170 Reward: 12.0 Epsilon 0.8289999999999998 mean q 9.215378\n",
      "Episode:  171 Reward: 18.0 Epsilon 0.8279999999999998 mean q 9.450025\n",
      "Episode:  172 Reward: 18.0 Epsilon 0.8269999999999998 mean q 9.460415\n",
      "Episode:  173 Reward: 24.0 Epsilon 0.8259999999999998 mean q 8.5078535\n",
      "Episode:  174 Reward: 20.0 Epsilon 0.8249999999999998 mean q 7.90484\n",
      "Episode:  175 Reward: 32.0 Epsilon 0.8239999999999998 mean q 9.242381\n",
      "Episode:  176 Reward: 10.0 Epsilon 0.8229999999999998 mean q 11.530444\n",
      "Episode:  177 Reward: 10.0 Epsilon 0.8219999999999998 mean q 12.720791\n",
      "Episode:  178 Reward: 36.0 Epsilon 0.8209999999999998 mean q 9.549485\n",
      "Episode:  179 Reward: 38.0 Epsilon 0.8199999999999998 mean q 9.7202\n",
      "Episode:  180 Reward: 11.0 Epsilon 0.8189999999999998 mean q 12.15718\n",
      "Episode:  181 Reward: 17.0 Epsilon 0.8179999999999998 mean q 9.610047\n",
      "Episode:  182 Reward: 16.0 Epsilon 0.8169999999999998 mean q 10.072718\n",
      "Episode:  183 Reward: 11.0 Epsilon 0.8159999999999998 mean q 12.259491\n",
      "Episode:  184 Reward: 13.0 Epsilon 0.8149999999999998 mean q 9.229879\n",
      "Episode:  185 Reward: 17.0 Epsilon 0.8139999999999998 mean q 10.526278\n",
      "Episode:  186 Reward: 15.0 Epsilon 0.8129999999999998 mean q 8.749015\n",
      "Episode:  187 Reward: 18.0 Epsilon 0.8119999999999998 mean q 9.825276\n",
      "Episode:  188 Reward: 37.0 Epsilon 0.8109999999999998 mean q 9.65861\n",
      "Episode:  189 Reward: 8.0 Epsilon 0.8099999999999998 mean q 13.149385\n",
      "Episode:  190 Reward: 21.0 Epsilon 0.8089999999999998 mean q 10.194615\n",
      "Episode:  191 Reward: 24.0 Epsilon 0.8079999999999998 mean q 9.163227\n",
      "Episode:  192 Reward: 9.0 Epsilon 0.8069999999999998 mean q 13.030035\n",
      "Episode:  193 Reward: 28.0 Epsilon 0.8059999999999998 mean q 9.330764\n",
      "Episode:  194 Reward: 14.0 Epsilon 0.8049999999999998 mean q 13.083044\n",
      "Episode:  195 Reward: 18.0 Epsilon 0.8039999999999998 mean q 11.722128\n",
      "Episode:  196 Reward: 39.0 Epsilon 0.8029999999999998 mean q 9.683356\n",
      "Episode:  197 Reward: 11.0 Epsilon 0.8019999999999998 mean q 12.230179\n",
      "Episode:  198 Reward: 10.0 Epsilon 0.8009999999999998 mean q 14.135353\n",
      "Episode:  199 Reward: 22.0 Epsilon 0.7999999999999998 mean q 11.259041\n",
      "Episode:  200 Reward: 21.0 Epsilon 0.7989999999999998 mean q 11.115298\n",
      "Episode:  201 Reward: 9.0 Epsilon 0.7979999999999998 mean q 13.615661\n",
      "Episode:  202 Reward: 10.0 Epsilon 0.7969999999999998 mean q 11.114207\n",
      "Episode:  203 Reward: 14.0 Epsilon 0.7959999999999998 mean q 12.465799\n",
      "Episode:  204 Reward: 14.0 Epsilon 0.7949999999999998 mean q 13.004256\n",
      "Episode:  205 Reward: 21.0 Epsilon 0.7939999999999998 mean q 12.107275\n",
      "Episode:  206 Reward: 13.0 Epsilon 0.7929999999999998 mean q 13.223223\n",
      "Episode:  207 Reward: 14.0 Epsilon 0.7919999999999998 mean q 12.105451\n",
      "Episode:  208 Reward: 11.0 Epsilon 0.7909999999999998 mean q 14.329972\n",
      "Episode:  209 Reward: 17.0 Epsilon 0.7899999999999998 mean q 11.509787\n",
      "Episode:  210 Reward: 11.0 Epsilon 0.7889999999999998 mean q 13.616031\n",
      "Episode:  211 Reward: 13.0 Epsilon 0.7879999999999998 mean q 12.783593\n",
      "Episode:  212 Reward: 14.0 Epsilon 0.7869999999999998 mean q 12.797854\n",
      "Episode:  213 Reward: 10.0 Epsilon 0.7859999999999998 mean q 15.248996\n",
      "Episode:  214 Reward: 9.0 Epsilon 0.7849999999999998 mean q 15.322006\n",
      "Episode:  215 Reward: 21.0 Epsilon 0.7839999999999998 mean q 11.498816\n",
      "Episode:  216 Reward: 14.0 Epsilon 0.7829999999999998 mean q 13.41985\n",
      "Episode:  217 Reward: 23.0 Epsilon 0.7819999999999998 mean q 12.260393\n",
      "Episode:  218 Reward: 11.0 Epsilon 0.7809999999999998 mean q 11.132114\n",
      "Episode:  219 Reward: 48.0 Epsilon 0.7799999999999998 mean q 12.9681015\n",
      "Episode:  220 Reward: 12.0 Epsilon 0.7789999999999998 mean q 14.36456\n",
      "Episode:  221 Reward: 21.0 Epsilon 0.7779999999999998 mean q 12.745428\n",
      "Episode:  222 Reward: 13.0 Epsilon 0.7769999999999998 mean q 14.620468\n",
      "Episode:  223 Reward: 16.0 Epsilon 0.7759999999999998 mean q 11.79771\n",
      "Episode:  224 Reward: 26.0 Epsilon 0.7749999999999998 mean q 12.166853\n",
      "Episode:  225 Reward: 11.0 Epsilon 0.7739999999999998 mean q 14.408475\n",
      "Episode:  226 Reward: 11.0 Epsilon 0.7729999999999998 mean q 15.525478\n",
      "Episode:  227 Reward: 16.0 Epsilon 0.7719999999999998 mean q 11.801911\n",
      "Episode:  228 Reward: 11.0 Epsilon 0.7709999999999998 mean q 16.0822\n",
      "Episode:  229 Reward: 20.0 Epsilon 0.7699999999999998 mean q 12.527989\n",
      "Episode:  230 Reward: 13.0 Epsilon 0.7689999999999998 mean q 15.488325\n",
      "Episode:  231 Reward: 15.0 Epsilon 0.7679999999999998 mean q 11.379208\n",
      "Episode:  232 Reward: 10.0 Epsilon 0.7669999999999998 mean q 15.259898\n",
      "Episode:  233 Reward: 17.0 Epsilon 0.7659999999999998 mean q 12.096721\n",
      "Episode:  234 Reward: 14.0 Epsilon 0.7649999999999998 mean q 13.854312\n",
      "Episode:  235 Reward: 12.0 Epsilon 0.7639999999999998 mean q 15.652396\n",
      "Episode:  236 Reward: 16.0 Epsilon 0.7629999999999998 mean q 14.053442\n",
      "Episode:  237 Reward: 26.0 Epsilon 0.7619999999999998 mean q 13.624486\n",
      "Episode:  238 Reward: 19.0 Epsilon 0.7609999999999998 mean q 13.570763\n",
      "Episode:  239 Reward: 16.0 Epsilon 0.7599999999999998 mean q 15.434395\n",
      "Episode:  240 Reward: 14.0 Epsilon 0.7589999999999998 mean q 14.004995\n",
      "Episode:  241 Reward: 16.0 Epsilon 0.7579999999999998 mean q 13.216469\n",
      "Episode:  242 Reward: 28.0 Epsilon 0.7569999999999998 mean q 13.974145\n",
      "Episode:  243 Reward: 15.0 Epsilon 0.7559999999999998 mean q 12.981988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  244 Reward: 19.0 Epsilon 0.7549999999999998 mean q 13.7049465\n",
      "Episode:  245 Reward: 17.0 Epsilon 0.7539999999999998 mean q 13.354324\n",
      "Episode:  246 Reward: 15.0 Epsilon 0.7529999999999998 mean q 13.612302\n",
      "Episode:  247 Reward: 13.0 Epsilon 0.7519999999999998 mean q 14.802767\n",
      "Episode:  248 Reward: 11.0 Epsilon 0.7509999999999998 mean q 16.259346\n",
      "Episode:  249 Reward: 36.0 Epsilon 0.7499999999999998 mean q 12.930091\n",
      "Episode:  250 Reward: 16.0 Epsilon 0.7489999999999998 mean q 13.643428\n",
      "Episode:  251 Reward: 48.0 Epsilon 0.7479999999999998 mean q 13.005387\n",
      "Episode:  252 Reward: 16.0 Epsilon 0.7469999999999998 mean q 14.082033\n",
      "Episode:  253 Reward: 11.0 Epsilon 0.7459999999999998 mean q 16.301714\n",
      "Episode:  254 Reward: 13.0 Epsilon 0.7449999999999998 mean q 15.342739\n",
      "Episode:  255 Reward: 37.0 Epsilon 0.7439999999999998 mean q 13.5175705\n",
      "Episode:  256 Reward: 9.0 Epsilon 0.7429999999999998 mean q 13.862559\n",
      "Episode:  257 Reward: 11.0 Epsilon 0.7419999999999998 mean q 13.760604\n",
      "Episode:  258 Reward: 13.0 Epsilon 0.7409999999999998 mean q 12.356969\n",
      "Episode:  259 Reward: 11.0 Epsilon 0.7399999999999998 mean q 13.533755\n",
      "Episode:  260 Reward: 24.0 Epsilon 0.7389999999999998 mean q 13.380188\n",
      "Episode:  261 Reward: 10.0 Epsilon 0.7379999999999998 mean q 15.183225\n",
      "Episode:  262 Reward: 17.0 Epsilon 0.7369999999999998 mean q 13.830719\n",
      "Episode:  263 Reward: 18.0 Epsilon 0.7359999999999998 mean q 12.468207\n",
      "Episode:  264 Reward: 18.0 Epsilon 0.7349999999999998 mean q 13.756597\n",
      "Episode:  265 Reward: 11.0 Epsilon 0.7339999999999998 mean q 13.714558\n",
      "Episode:  266 Reward: 13.0 Epsilon 0.7329999999999998 mean q 13.601487\n",
      "Episode:  267 Reward: 17.0 Epsilon 0.7319999999999998 mean q 12.896304\n",
      "Episode:  268 Reward: 23.0 Epsilon 0.7309999999999998 mean q 12.549648\n",
      "Episode:  269 Reward: 16.0 Epsilon 0.7299999999999998 mean q 13.090084\n",
      "Episode:  270 Reward: 39.0 Epsilon 0.7289999999999998 mean q 12.718443\n",
      "Episode:  271 Reward: 13.0 Epsilon 0.7279999999999998 mean q 13.264952\n",
      "Episode:  272 Reward: 11.0 Epsilon 0.7269999999999998 mean q 14.50627\n",
      "Episode:  273 Reward: 14.0 Epsilon 0.7259999999999998 mean q 12.8347025\n",
      "Episode:  274 Reward: 13.0 Epsilon 0.7249999999999998 mean q 12.639434\n",
      "Episode:  275 Reward: 27.0 Epsilon 0.7239999999999998 mean q 13.570628\n",
      "Episode:  276 Reward: 11.0 Epsilon 0.7229999999999998 mean q 14.010058\n",
      "Episode:  277 Reward: 21.0 Epsilon 0.7219999999999998 mean q 13.38145\n",
      "Episode:  278 Reward: 10.0 Epsilon 0.7209999999999998 mean q 14.203488\n",
      "Episode:  279 Reward: 11.0 Epsilon 0.7199999999999998 mean q 13.690579\n",
      "Episode:  280 Reward: 10.0 Epsilon 0.7189999999999998 mean q 14.50413\n",
      "Episode:  281 Reward: 23.0 Epsilon 0.7179999999999997 mean q 13.041046\n",
      "Episode:  282 Reward: 9.0 Epsilon 0.7169999999999997 mean q 13.85223\n",
      "Episode:  283 Reward: 15.0 Epsilon 0.7159999999999997 mean q 13.163113\n",
      "Episode:  284 Reward: 14.0 Epsilon 0.7149999999999997 mean q 12.519584\n",
      "Episode:  285 Reward: 28.0 Epsilon 0.7139999999999997 mean q 12.699904\n",
      "Episode:  286 Reward: 23.0 Epsilon 0.7129999999999997 mean q 12.653765\n",
      "Episode:  287 Reward: 11.0 Epsilon 0.7119999999999997 mean q 13.622837\n",
      "Episode:  288 Reward: 21.0 Epsilon 0.7109999999999997 mean q 12.859939\n",
      "Episode:  289 Reward: 18.0 Epsilon 0.7099999999999997 mean q 12.439255\n",
      "Episode:  290 Reward: 17.0 Epsilon 0.7089999999999997 mean q 12.171691\n",
      "Episode:  291 Reward: 10.0 Epsilon 0.7079999999999997 mean q 13.387988\n",
      "Episode:  292 Reward: 17.0 Epsilon 0.7069999999999997 mean q 13.0987015\n",
      "Episode:  293 Reward: 10.0 Epsilon 0.7059999999999997 mean q 12.780429\n",
      "Episode:  294 Reward: 9.0 Epsilon 0.7049999999999997 mean q 13.7509985\n",
      "Episode:  295 Reward: 14.0 Epsilon 0.7039999999999997 mean q 12.523311\n",
      "Episode:  296 Reward: 10.0 Epsilon 0.7029999999999997 mean q 13.439821\n",
      "Episode:  297 Reward: 12.0 Epsilon 0.7019999999999997 mean q 12.55255\n",
      "Episode:  298 Reward: 35.0 Epsilon 0.7009999999999997 mean q 12.156154\n",
      "Episode:  299 Reward: 14.0 Epsilon 0.6999999999999997 mean q 13.072629\n",
      "Episode:  300 Reward: 15.0 Epsilon 0.6989999999999997 mean q 12.275452\n",
      "Episode:  301 Reward: 15.0 Epsilon 0.6979999999999997 mean q 12.2787\n",
      "Episode:  302 Reward: 19.0 Epsilon 0.6969999999999997 mean q 12.267295\n",
      "Episode:  303 Reward: 9.0 Epsilon 0.6959999999999997 mean q 12.506824\n",
      "Episode:  304 Reward: 11.0 Epsilon 0.6949999999999997 mean q 12.597698\n",
      "Episode:  305 Reward: 10.0 Epsilon 0.6939999999999997 mean q 12.929541\n",
      "Episode:  306 Reward: 25.0 Epsilon 0.6929999999999997 mean q 11.940125\n",
      "Episode:  307 Reward: 12.0 Epsilon 0.6919999999999997 mean q 12.36977\n",
      "Episode:  308 Reward: 11.0 Epsilon 0.6909999999999997 mean q 12.172782\n",
      "Episode:  309 Reward: 13.0 Epsilon 0.6899999999999997 mean q 12.477808\n",
      "Episode:  310 Reward: 13.0 Epsilon 0.6889999999999997 mean q 12.189473\n",
      "Episode:  311 Reward: 10.0 Epsilon 0.6879999999999997 mean q 14.005201\n",
      "Episode:  312 Reward: 24.0 Epsilon 0.6869999999999997 mean q 12.751701\n",
      "Episode:  313 Reward: 22.0 Epsilon 0.6859999999999997 mean q 12.258117\n",
      "Episode:  314 Reward: 22.0 Epsilon 0.6849999999999997 mean q 12.28366\n",
      "Episode:  315 Reward: 17.0 Epsilon 0.6839999999999997 mean q 11.683363\n",
      "Episode:  316 Reward: 10.0 Epsilon 0.6829999999999997 mean q 12.775629\n",
      "Episode:  317 Reward: 10.0 Epsilon 0.6819999999999997 mean q 11.748554\n",
      "Episode:  318 Reward: 11.0 Epsilon 0.6809999999999997 mean q 12.836379\n",
      "Episode:  319 Reward: 10.0 Epsilon 0.6799999999999997 mean q 12.92264\n",
      "Episode:  320 Reward: 13.0 Epsilon 0.6789999999999997 mean q 12.813822\n",
      "Episode:  321 Reward: 17.0 Epsilon 0.6779999999999997 mean q 12.307006\n",
      "Episode:  322 Reward: 35.0 Epsilon 0.6769999999999997 mean q 12.263619\n",
      "Episode:  323 Reward: 17.0 Epsilon 0.6759999999999997 mean q 12.382946\n",
      "Episode:  324 Reward: 12.0 Epsilon 0.6749999999999997 mean q 13.106059\n",
      "Episode:  325 Reward: 19.0 Epsilon 0.6739999999999997 mean q 12.023341\n",
      "Episode:  326 Reward: 21.0 Epsilon 0.6729999999999997 mean q 12.79807\n",
      "Episode:  327 Reward: 10.0 Epsilon 0.6719999999999997 mean q 12.911638\n",
      "Episode:  328 Reward: 37.0 Epsilon 0.6709999999999997 mean q 11.646867\n",
      "Episode:  329 Reward: 20.0 Epsilon 0.6699999999999997 mean q 12.276026\n",
      "Episode:  330 Reward: 18.0 Epsilon 0.6689999999999997 mean q 12.164494\n",
      "Episode:  331 Reward: 8.0 Epsilon 0.6679999999999997 mean q 13.062159\n",
      "Episode:  332 Reward: 14.0 Epsilon 0.6669999999999997 mean q 11.239233\n",
      "Episode:  333 Reward: 16.0 Epsilon 0.6659999999999997 mean q 12.6354475\n",
      "Episode:  334 Reward: 11.0 Epsilon 0.6649999999999997 mean q 12.729797\n",
      "Episode:  335 Reward: 10.0 Epsilon 0.6639999999999997 mean q 11.242189\n",
      "Episode:  336 Reward: 10.0 Epsilon 0.6629999999999997 mean q 12.999197\n",
      "Episode:  337 Reward: 13.0 Epsilon 0.6619999999999997 mean q 12.639211\n",
      "Episode:  338 Reward: 10.0 Epsilon 0.6609999999999997 mean q 13.908809\n",
      "Episode:  339 Reward: 15.0 Epsilon 0.6599999999999997 mean q 12.798236\n",
      "Episode:  340 Reward: 20.0 Epsilon 0.6589999999999997 mean q 11.146529\n",
      "Episode:  341 Reward: 16.0 Epsilon 0.6579999999999997 mean q 12.532028\n",
      "Episode:  342 Reward: 21.0 Epsilon 0.6569999999999997 mean q 11.97132\n",
      "Episode:  343 Reward: 12.0 Epsilon 0.6559999999999997 mean q 13.186614\n",
      "Episode:  344 Reward: 9.0 Epsilon 0.6549999999999997 mean q 13.09914\n",
      "Episode:  345 Reward: 18.0 Epsilon 0.6539999999999997 mean q 12.0527525\n",
      "Episode:  346 Reward: 11.0 Epsilon 0.6529999999999997 mean q 12.911037\n",
      "Episode:  347 Reward: 13.0 Epsilon 0.6519999999999997 mean q 12.335942\n",
      "Episode:  348 Reward: 13.0 Epsilon 0.6509999999999997 mean q 12.4709\n",
      "Episode:  349 Reward: 14.0 Epsilon 0.6499999999999997 mean q 12.541288\n",
      "Episode:  350 Reward: 14.0 Epsilon 0.6489999999999997 mean q 11.021273\n",
      "Episode:  351 Reward: 17.0 Epsilon 0.6479999999999997 mean q 12.718572\n",
      "Episode:  352 Reward: 13.0 Epsilon 0.6469999999999997 mean q 12.744986\n",
      "Episode:  353 Reward: 12.0 Epsilon 0.6459999999999997 mean q 12.375188\n",
      "Episode:  354 Reward: 19.0 Epsilon 0.6449999999999997 mean q 12.107205\n",
      "Episode:  355 Reward: 13.0 Epsilon 0.6439999999999997 mean q 12.393846\n",
      "Episode:  356 Reward: 14.0 Epsilon 0.6429999999999997 mean q 12.627009\n",
      "Episode:  357 Reward: 13.0 Epsilon 0.6419999999999997 mean q 12.834542\n",
      "Episode:  358 Reward: 21.0 Epsilon 0.6409999999999997 mean q 12.403811\n",
      "Episode:  359 Reward: 9.0 Epsilon 0.6399999999999997 mean q 13.224791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  360 Reward: 24.0 Epsilon 0.6389999999999997 mean q 11.043225\n",
      "Episode:  361 Reward: 11.0 Epsilon 0.6379999999999997 mean q 13.274247\n",
      "Episode:  362 Reward: 16.0 Epsilon 0.6369999999999997 mean q 12.3041935\n",
      "Episode:  363 Reward: 10.0 Epsilon 0.6359999999999997 mean q 13.524132\n",
      "Episode:  364 Reward: 19.0 Epsilon 0.6349999999999997 mean q 12.749582\n",
      "Episode:  365 Reward: 13.0 Epsilon 0.6339999999999997 mean q 12.753703\n",
      "Episode:  366 Reward: 11.0 Epsilon 0.6329999999999997 mean q 12.635136\n",
      "Episode:  367 Reward: 11.0 Epsilon 0.6319999999999997 mean q 12.692767\n",
      "Episode:  368 Reward: 16.0 Epsilon 0.6309999999999997 mean q 12.212349\n",
      "Episode:  369 Reward: 17.0 Epsilon 0.6299999999999997 mean q 12.371934\n",
      "Episode:  370 Reward: 27.0 Epsilon 0.6289999999999997 mean q 12.471139\n",
      "Episode:  371 Reward: 11.0 Epsilon 0.6279999999999997 mean q 13.345877\n",
      "Episode:  372 Reward: 22.0 Epsilon 0.6269999999999997 mean q 12.4395685\n",
      "Episode:  373 Reward: 16.0 Epsilon 0.6259999999999997 mean q 12.582109\n",
      "Episode:  374 Reward: 21.0 Epsilon 0.6249999999999997 mean q 12.233007\n",
      "Episode:  375 Reward: 9.0 Epsilon 0.6239999999999997 mean q 13.22765\n",
      "Episode:  376 Reward: 17.0 Epsilon 0.6229999999999997 mean q 12.30622\n",
      "Episode:  377 Reward: 10.0 Epsilon 0.6219999999999997 mean q 13.656049\n",
      "Episode:  378 Reward: 10.0 Epsilon 0.6209999999999997 mean q 12.789835\n",
      "Episode:  379 Reward: 9.0 Epsilon 0.6199999999999997 mean q 13.342135\n",
      "Episode:  380 Reward: 15.0 Epsilon 0.6189999999999997 mean q 12.8327675\n",
      "Episode:  381 Reward: 16.0 Epsilon 0.6179999999999997 mean q 12.769092\n",
      "Episode:  382 Reward: 21.0 Epsilon 0.6169999999999997 mean q 12.487502\n",
      "Episode:  383 Reward: 14.0 Epsilon 0.6159999999999997 mean q 13.158536\n",
      "Episode:  384 Reward: 14.0 Epsilon 0.6149999999999997 mean q 12.452103\n",
      "Episode:  385 Reward: 15.0 Epsilon 0.6139999999999997 mean q 12.517947\n",
      "Episode:  386 Reward: 21.0 Epsilon 0.6129999999999997 mean q 12.580424\n",
      "Episode:  387 Reward: 14.0 Epsilon 0.6119999999999997 mean q 12.724455\n",
      "Episode:  388 Reward: 13.0 Epsilon 0.6109999999999997 mean q 13.243937\n",
      "Episode:  389 Reward: 12.0 Epsilon 0.6099999999999997 mean q 12.918576\n",
      "Episode:  390 Reward: 28.0 Epsilon 0.6089999999999997 mean q 12.467265\n",
      "Episode:  391 Reward: 9.0 Epsilon 0.6079999999999997 mean q 12.758689\n",
      "Episode:  392 Reward: 13.0 Epsilon 0.6069999999999997 mean q 12.674763\n",
      "Episode:  393 Reward: 20.0 Epsilon 0.6059999999999997 mean q 12.339982\n",
      "Episode:  394 Reward: 12.0 Epsilon 0.6049999999999996 mean q 12.849075\n",
      "Episode:  395 Reward: 10.0 Epsilon 0.6039999999999996 mean q 9.986056\n",
      "Episode:  396 Reward: 10.0 Epsilon 0.6029999999999996 mean q 12.814423\n",
      "Episode:  397 Reward: 19.0 Epsilon 0.6019999999999996 mean q 12.143564\n",
      "Episode:  398 Reward: 12.0 Epsilon 0.6009999999999996 mean q 12.701683\n",
      "Episode:  399 Reward: 13.0 Epsilon 0.5999999999999996 mean q 10.500193\n",
      "Episode:  400 Reward: 10.0 Epsilon 0.5989999999999996 mean q 12.538361\n",
      "Episode:  401 Reward: 14.0 Epsilon 0.5979999999999996 mean q 12.530896\n",
      "Episode:  402 Reward: 12.0 Epsilon 0.5969999999999996 mean q 12.249328\n",
      "Episode:  403 Reward: 18.0 Epsilon 0.5959999999999996 mean q 12.310211\n",
      "Episode:  404 Reward: 12.0 Epsilon 0.5949999999999996 mean q 12.4644165\n",
      "Episode:  405 Reward: 11.0 Epsilon 0.5939999999999996 mean q 12.543384\n",
      "Episode:  406 Reward: 20.0 Epsilon 0.5929999999999996 mean q 11.609924\n",
      "Episode:  407 Reward: 9.0 Epsilon 0.5919999999999996 mean q 12.286922\n",
      "Episode:  408 Reward: 13.0 Epsilon 0.5909999999999996 mean q 11.932149\n",
      "Episode:  409 Reward: 14.0 Epsilon 0.5899999999999996 mean q 12.088724\n",
      "Episode:  410 Reward: 13.0 Epsilon 0.5889999999999996 mean q 12.10442\n",
      "Episode:  411 Reward: 21.0 Epsilon 0.5879999999999996 mean q 11.799122\n",
      "Episode:  412 Reward: 14.0 Epsilon 0.5869999999999996 mean q 11.907999\n",
      "Episode:  413 Reward: 16.0 Epsilon 0.5859999999999996 mean q 11.74909\n",
      "Episode:  414 Reward: 28.0 Epsilon 0.5849999999999996 mean q 11.643029\n",
      "Episode:  415 Reward: 14.0 Epsilon 0.5839999999999996 mean q 11.960289\n",
      "Episode:  416 Reward: 11.0 Epsilon 0.5829999999999996 mean q 11.830911\n",
      "Episode:  417 Reward: 20.0 Epsilon 0.5819999999999996 mean q 11.467616\n",
      "Episode:  418 Reward: 21.0 Epsilon 0.5809999999999996 mean q 11.5910015\n",
      "Episode:  419 Reward: 10.0 Epsilon 0.5799999999999996 mean q 11.687726\n",
      "Episode:  420 Reward: 14.0 Epsilon 0.5789999999999996 mean q 11.643951\n",
      "Episode:  421 Reward: 10.0 Epsilon 0.5779999999999996 mean q 11.595648\n",
      "Episode:  422 Reward: 11.0 Epsilon 0.5769999999999996 mean q 11.559121\n",
      "Episode:  423 Reward: 16.0 Epsilon 0.5759999999999996 mean q 11.550879\n",
      "Episode:  424 Reward: 10.0 Epsilon 0.5749999999999996 mean q 11.883919\n",
      "Episode:  425 Reward: 18.0 Epsilon 0.5739999999999996 mean q 11.499104\n",
      "Episode:  426 Reward: 9.0 Epsilon 0.5729999999999996 mean q 11.468743\n",
      "Episode:  427 Reward: 13.0 Epsilon 0.5719999999999996 mean q 11.363027\n",
      "Episode:  428 Reward: 13.0 Epsilon 0.5709999999999996 mean q 11.431601\n",
      "Episode:  429 Reward: 9.0 Epsilon 0.5699999999999996 mean q 11.62291\n",
      "Episode:  430 Reward: 19.0 Epsilon 0.5689999999999996 mean q 11.353274\n",
      "Episode:  431 Reward: 11.0 Epsilon 0.5679999999999996 mean q 11.184521\n",
      "Episode:  432 Reward: 11.0 Epsilon 0.5669999999999996 mean q 11.242827\n",
      "Episode:  433 Reward: 16.0 Epsilon 0.5659999999999996 mean q 11.097276\n",
      "Episode:  434 Reward: 17.0 Epsilon 0.5649999999999996 mean q 11.07684\n",
      "Episode:  435 Reward: 13.0 Epsilon 0.5639999999999996 mean q 11.183685\n",
      "Episode:  436 Reward: 13.0 Epsilon 0.5629999999999996 mean q 11.125101\n",
      "Episode:  437 Reward: 16.0 Epsilon 0.5619999999999996 mean q 10.987307\n",
      "Episode:  438 Reward: 17.0 Epsilon 0.5609999999999996 mean q 9.644846\n",
      "Episode:  439 Reward: 9.0 Epsilon 0.5599999999999996 mean q 10.762347\n",
      "Episode:  440 Reward: 14.0 Epsilon 0.5589999999999996 mean q 10.860879\n",
      "Episode:  441 Reward: 9.0 Epsilon 0.5579999999999996 mean q 10.6778755\n",
      "Episode:  442 Reward: 12.0 Epsilon 0.5569999999999996 mean q 10.607406\n",
      "Episode:  443 Reward: 19.0 Epsilon 0.5559999999999996 mean q 10.4863615\n",
      "Episode:  444 Reward: 13.0 Epsilon 0.5549999999999996 mean q 10.483327\n",
      "Episode:  445 Reward: 11.0 Epsilon 0.5539999999999996 mean q 10.470725\n",
      "Episode:  446 Reward: 10.0 Epsilon 0.5529999999999996 mean q 10.4269\n",
      "Episode:  447 Reward: 14.0 Epsilon 0.5519999999999996 mean q 10.572017\n",
      "Episode:  448 Reward: 13.0 Epsilon 0.5509999999999996 mean q 10.518137\n",
      "Episode:  449 Reward: 10.0 Epsilon 0.5499999999999996 mean q 10.432335\n",
      "Episode:  450 Reward: 12.0 Epsilon 0.5489999999999996 mean q 10.278922\n",
      "Episode:  451 Reward: 10.0 Epsilon 0.5479999999999996 mean q 10.283423\n",
      "Episode:  452 Reward: 14.0 Epsilon 0.5469999999999996 mean q 10.147797\n",
      "Episode:  453 Reward: 37.0 Epsilon 0.5459999999999996 mean q 10.283235\n",
      "Episode:  454 Reward: 10.0 Epsilon 0.5449999999999996 mean q 10.123464\n",
      "Episode:  455 Reward: 14.0 Epsilon 0.5439999999999996 mean q 10.118838\n",
      "Episode:  456 Reward: 14.0 Epsilon 0.5429999999999996 mean q 9.978102\n",
      "Episode:  457 Reward: 14.0 Epsilon 0.5419999999999996 mean q 10.057465\n",
      "Episode:  458 Reward: 14.0 Epsilon 0.5409999999999996 mean q 10.11109\n",
      "Episode:  459 Reward: 11.0 Epsilon 0.5399999999999996 mean q 9.823579\n",
      "Episode:  460 Reward: 12.0 Epsilon 0.5389999999999996 mean q 9.885444\n",
      "Episode:  461 Reward: 9.0 Epsilon 0.5379999999999996 mean q 9.872586\n",
      "Episode:  462 Reward: 12.0 Epsilon 0.5369999999999996 mean q 9.658965\n",
      "Episode:  463 Reward: 13.0 Epsilon 0.5359999999999996 mean q 9.826775\n",
      "Episode:  464 Reward: 14.0 Epsilon 0.5349999999999996 mean q 9.877826\n",
      "Episode:  465 Reward: 12.0 Epsilon 0.5339999999999996 mean q 9.751891\n",
      "Episode:  466 Reward: 11.0 Epsilon 0.5329999999999996 mean q 9.6923485\n",
      "Episode:  467 Reward: 22.0 Epsilon 0.5319999999999996 mean q 9.960715\n",
      "Episode:  468 Reward: 13.0 Epsilon 0.5309999999999996 mean q 9.487814\n",
      "Episode:  469 Reward: 11.0 Epsilon 0.5299999999999996 mean q 9.66088\n",
      "Episode:  470 Reward: 11.0 Epsilon 0.5289999999999996 mean q 9.452539\n",
      "Episode:  471 Reward: 19.0 Epsilon 0.5279999999999996 mean q 9.419727\n",
      "Episode:  472 Reward: 14.0 Epsilon 0.5269999999999996 mean q 9.513986\n",
      "Episode:  473 Reward: 10.0 Epsilon 0.5259999999999996 mean q 8.98219\n",
      "Episode:  474 Reward: 13.0 Epsilon 0.5249999999999996 mean q 8.987904\n",
      "Episode:  475 Reward: 20.0 Epsilon 0.5239999999999996 mean q 9.313048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  476 Reward: 19.0 Epsilon 0.5229999999999996 mean q 9.151909\n",
      "Episode:  477 Reward: 10.0 Epsilon 0.5219999999999996 mean q 8.823646\n",
      "Episode:  478 Reward: 11.0 Epsilon 0.5209999999999996 mean q 8.631983\n",
      "Episode:  479 Reward: 11.0 Epsilon 0.5199999999999996 mean q 8.992942\n",
      "Episode:  480 Reward: 12.0 Epsilon 0.5189999999999996 mean q 8.2557335\n",
      "Episode:  481 Reward: 30.0 Epsilon 0.5179999999999996 mean q 9.007909\n",
      "Episode:  482 Reward: 12.0 Epsilon 0.5169999999999996 mean q 8.285149\n",
      "Episode:  483 Reward: 13.0 Epsilon 0.5159999999999996 mean q 8.780474\n",
      "Episode:  484 Reward: 21.0 Epsilon 0.5149999999999996 mean q 8.876531\n",
      "Episode:  485 Reward: 11.0 Epsilon 0.5139999999999996 mean q 8.267873\n",
      "Episode:  486 Reward: 13.0 Epsilon 0.5129999999999996 mean q 8.440977\n",
      "Episode:  487 Reward: 13.0 Epsilon 0.5119999999999996 mean q 8.34956\n",
      "Episode:  488 Reward: 9.0 Epsilon 0.5109999999999996 mean q 7.967817\n",
      "Episode:  489 Reward: 11.0 Epsilon 0.5099999999999996 mean q 8.092477\n",
      "Episode:  490 Reward: 13.0 Epsilon 0.5089999999999996 mean q 8.80575\n",
      "Episode:  491 Reward: 13.0 Epsilon 0.5079999999999996 mean q 8.555518\n",
      "Episode:  492 Reward: 10.0 Epsilon 0.5069999999999996 mean q 8.048373\n",
      "Episode:  493 Reward: 30.0 Epsilon 0.5059999999999996 mean q 9.105837\n",
      "Episode:  494 Reward: 13.0 Epsilon 0.5049999999999996 mean q 8.65653\n",
      "Episode:  495 Reward: 12.0 Epsilon 0.5039999999999996 mean q 7.989076\n",
      "Episode:  496 Reward: 20.0 Epsilon 0.5029999999999996 mean q 8.858441\n",
      "Episode:  497 Reward: 12.0 Epsilon 0.5019999999999996 mean q 8.552636\n",
      "Episode:  498 Reward: 10.0 Epsilon 0.5009999999999996 mean q 7.9727836\n",
      "Episode:  499 Reward: 9.0 Epsilon 0.49999999999999956 mean q 8.08836\n",
      "Episode:  500 Reward: 14.0 Epsilon 0.49899999999999956 mean q 8.241824\n",
      "Episode:  501 Reward: 10.0 Epsilon 0.49799999999999955 mean q 8.476191\n",
      "Episode:  502 Reward: 20.0 Epsilon 0.49699999999999955 mean q 8.453989\n",
      "Episode:  503 Reward: 68.0 Epsilon 0.49599999999999955 mean q 8.919911\n",
      "Episode:  504 Reward: 23.0 Epsilon 0.49499999999999955 mean q 8.860222\n",
      "Episode:  505 Reward: 19.0 Epsilon 0.49399999999999955 mean q 8.657153\n",
      "Episode:  506 Reward: 23.0 Epsilon 0.49299999999999955 mean q 8.829556\n",
      "Episode:  507 Reward: 22.0 Epsilon 0.49199999999999955 mean q 8.933007\n",
      "Episode:  508 Reward: 52.0 Epsilon 0.49099999999999955 mean q 9.184879\n",
      "Episode:  509 Reward: 23.0 Epsilon 0.48999999999999955 mean q 9.001975\n",
      "Episode:  510 Reward: 35.0 Epsilon 0.48899999999999955 mean q 9.279726\n",
      "Episode:  511 Reward: 113.0 Epsilon 0.48799999999999955 mean q 9.275536\n",
      "Episode:  512 Reward: 59.0 Epsilon 0.48699999999999954 mean q 9.589954\n",
      "Episode:  513 Reward: 29.0 Epsilon 0.48599999999999954 mean q 9.6899185\n",
      "Episode:  514 Reward: 52.0 Epsilon 0.48499999999999954 mean q 9.673603\n",
      "Episode:  515 Reward: 54.0 Epsilon 0.48399999999999954 mean q 10.036794\n",
      "Episode:  516 Reward: 24.0 Epsilon 0.48299999999999954 mean q 9.921226\n",
      "Episode:  517 Reward: 73.0 Epsilon 0.48199999999999954 mean q 10.324904\n",
      "Episode:  518 Reward: 38.0 Epsilon 0.48099999999999954 mean q 10.168392\n",
      "Episode:  519 Reward: 26.0 Epsilon 0.47999999999999954 mean q 10.019167\n",
      "Episode:  520 Reward: 35.0 Epsilon 0.47899999999999954 mean q 9.87892\n",
      "Episode:  521 Reward: 55.0 Epsilon 0.47799999999999954 mean q 10.071358\n",
      "Episode:  522 Reward: 38.0 Epsilon 0.47699999999999954 mean q 10.454122\n",
      "Episode:  523 Reward: 16.0 Epsilon 0.47599999999999953 mean q 9.367559\n",
      "Episode:  524 Reward: 50.0 Epsilon 0.47499999999999953 mean q 10.569884\n",
      "Episode:  525 Reward: 35.0 Epsilon 0.47399999999999953 mean q 10.281089\n",
      "Episode:  526 Reward: 50.0 Epsilon 0.47299999999999953 mean q 10.60005\n",
      "Episode:  527 Reward: 70.0 Epsilon 0.47199999999999953 mean q 11.021519\n",
      "Episode:  528 Reward: 42.0 Epsilon 0.47099999999999953 mean q 10.669674\n",
      "Episode:  529 Reward: 42.0 Epsilon 0.46999999999999953 mean q 10.85741\n",
      "Episode:  530 Reward: 71.0 Epsilon 0.46899999999999953 mean q 11.501197\n",
      "Episode:  531 Reward: 32.0 Epsilon 0.4679999999999995 mean q 11.426654\n",
      "Episode:  532 Reward: 72.0 Epsilon 0.4669999999999995 mean q 11.44939\n",
      "Episode:  533 Reward: 15.0 Epsilon 0.4659999999999995 mean q 10.281122\n",
      "Episode:  534 Reward: 60.0 Epsilon 0.4649999999999995 mean q 11.76694\n",
      "Episode:  535 Reward: 49.0 Epsilon 0.4639999999999995 mean q 11.409554\n",
      "Episode:  536 Reward: 46.0 Epsilon 0.4629999999999995 mean q 11.978018\n",
      "Episode:  537 Reward: 40.0 Epsilon 0.4619999999999995 mean q 11.685552\n",
      "Episode:  538 Reward: 53.0 Epsilon 0.4609999999999995 mean q 11.835259\n",
      "Episode:  539 Reward: 35.0 Epsilon 0.4599999999999995 mean q 12.272283\n",
      "Episode:  540 Reward: 34.0 Epsilon 0.4589999999999995 mean q 11.858086\n",
      "Episode:  541 Reward: 40.0 Epsilon 0.4579999999999995 mean q 11.933828\n",
      "Episode:  542 Reward: 52.0 Epsilon 0.4569999999999995 mean q 12.281926\n",
      "Episode:  543 Reward: 38.0 Epsilon 0.4559999999999995 mean q 12.414136\n",
      "Episode:  544 Reward: 104.0 Epsilon 0.4549999999999995 mean q 12.547767\n",
      "Episode:  545 Reward: 43.0 Epsilon 0.4539999999999995 mean q 12.458028\n",
      "Episode:  546 Reward: 68.0 Epsilon 0.4529999999999995 mean q 12.940122\n",
      "Episode:  547 Reward: 65.0 Epsilon 0.4519999999999995 mean q 12.870299\n",
      "Episode:  548 Reward: 48.0 Epsilon 0.4509999999999995 mean q 12.779632\n",
      "Episode:  549 Reward: 47.0 Epsilon 0.4499999999999995 mean q 12.826868\n",
      "Episode:  550 Reward: 49.0 Epsilon 0.4489999999999995 mean q 12.90806\n",
      "Episode:  551 Reward: 70.0 Epsilon 0.4479999999999995 mean q 13.049768\n",
      "Episode:  552 Reward: 38.0 Epsilon 0.4469999999999995 mean q 13.142011\n",
      "Episode:  553 Reward: 73.0 Epsilon 0.4459999999999995 mean q 13.985689\n",
      "Episode:  554 Reward: 73.0 Epsilon 0.4449999999999995 mean q 13.676625\n",
      "Episode:  555 Reward: 64.0 Epsilon 0.4439999999999995 mean q 13.898466\n",
      "Episode:  556 Reward: 51.0 Epsilon 0.4429999999999995 mean q 13.70496\n",
      "Episode:  557 Reward: 48.0 Epsilon 0.4419999999999995 mean q 13.710513\n",
      "Episode:  558 Reward: 39.0 Epsilon 0.4409999999999995 mean q 13.52032\n",
      "Episode:  559 Reward: 40.0 Epsilon 0.4399999999999995 mean q 14.06366\n",
      "Episode:  560 Reward: 39.0 Epsilon 0.4389999999999995 mean q 14.018057\n",
      "Episode:  561 Reward: 64.0 Epsilon 0.4379999999999995 mean q 14.1666155\n",
      "Episode:  562 Reward: 47.0 Epsilon 0.4369999999999995 mean q 14.177342\n",
      "Episode:  563 Reward: 63.0 Epsilon 0.4359999999999995 mean q 13.902459\n",
      "Episode:  564 Reward: 72.0 Epsilon 0.4349999999999995 mean q 14.408342\n",
      "Episode:  565 Reward: 67.0 Epsilon 0.4339999999999995 mean q 14.859707\n",
      "Episode:  566 Reward: 29.0 Epsilon 0.4329999999999995 mean q 14.479022\n",
      "Episode:  567 Reward: 32.0 Epsilon 0.4319999999999995 mean q 14.348749\n",
      "Episode:  568 Reward: 24.0 Epsilon 0.4309999999999995 mean q 13.833577\n",
      "Episode:  569 Reward: 47.0 Epsilon 0.4299999999999995 mean q 14.52614\n",
      "Episode:  570 Reward: 42.0 Epsilon 0.4289999999999995 mean q 14.546983\n",
      "Episode:  571 Reward: 78.0 Epsilon 0.4279999999999995 mean q 15.2345295\n",
      "Episode:  572 Reward: 50.0 Epsilon 0.4269999999999995 mean q 14.726437\n",
      "Episode:  573 Reward: 54.0 Epsilon 0.4259999999999995 mean q 14.594855\n",
      "Episode:  574 Reward: 59.0 Epsilon 0.4249999999999995 mean q 15.301225\n",
      "Episode:  575 Reward: 48.0 Epsilon 0.4239999999999995 mean q 14.951747\n",
      "Episode:  576 Reward: 45.0 Epsilon 0.4229999999999995 mean q 14.380862\n",
      "Episode:  577 Reward: 56.0 Epsilon 0.4219999999999995 mean q 14.5921545\n",
      "Episode:  578 Reward: 42.0 Epsilon 0.4209999999999995 mean q 14.709612\n",
      "Episode:  579 Reward: 13.0 Epsilon 0.4199999999999995 mean q 13.054068\n",
      "Episode:  580 Reward: 53.0 Epsilon 0.4189999999999995 mean q 14.743063\n",
      "Episode:  581 Reward: 37.0 Epsilon 0.4179999999999995 mean q 14.561917\n",
      "Episode:  582 Reward: 10.0 Epsilon 0.4169999999999995 mean q 11.56847\n",
      "Episode:  583 Reward: 87.0 Epsilon 0.4159999999999995 mean q 15.619173\n",
      "Episode:  584 Reward: 39.0 Epsilon 0.4149999999999995 mean q 14.75897\n",
      "Episode:  585 Reward: 46.0 Epsilon 0.4139999999999995 mean q 14.960992\n",
      "Episode:  586 Reward: 57.0 Epsilon 0.4129999999999995 mean q 14.97615\n",
      "Episode:  587 Reward: 36.0 Epsilon 0.4119999999999995 mean q 14.872223\n",
      "Episode:  588 Reward: 30.0 Epsilon 0.4109999999999995 mean q 14.55124\n",
      "Episode:  589 Reward: 92.0 Epsilon 0.4099999999999995 mean q 15.6343155\n",
      "Episode:  590 Reward: 42.0 Epsilon 0.4089999999999995 mean q 15.183081\n",
      "Episode:  591 Reward: 46.0 Epsilon 0.4079999999999995 mean q 15.704017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  592 Reward: 109.0 Epsilon 0.4069999999999995 mean q 15.552663\n",
      "Episode:  593 Reward: 96.0 Epsilon 0.4059999999999995 mean q 15.367046\n",
      "Episode:  594 Reward: 51.0 Epsilon 0.40499999999999947 mean q 15.715783\n",
      "Episode:  595 Reward: 50.0 Epsilon 0.40399999999999947 mean q 14.991914\n",
      "Episode:  596 Reward: 68.0 Epsilon 0.40299999999999947 mean q 15.769242\n",
      "Episode:  597 Reward: 48.0 Epsilon 0.40199999999999947 mean q 15.18731\n",
      "Episode:  598 Reward: 31.0 Epsilon 0.40099999999999947 mean q 14.937833\n",
      "Episode:  599 Reward: 39.0 Epsilon 0.39999999999999947 mean q 15.106753\n",
      "Episode:  600 Reward: 106.0 Epsilon 0.39899999999999947 mean q 15.649923\n",
      "Episode:  601 Reward: 97.0 Epsilon 0.39799999999999947 mean q 15.851137\n",
      "Episode:  602 Reward: 33.0 Epsilon 0.39699999999999946 mean q 14.924986\n",
      "Episode:  603 Reward: 50.0 Epsilon 0.39599999999999946 mean q 15.590586\n",
      "Episode:  604 Reward: 55.0 Epsilon 0.39499999999999946 mean q 15.839267\n",
      "Episode:  605 Reward: 52.0 Epsilon 0.39399999999999946 mean q 15.388712\n",
      "Episode:  606 Reward: 48.0 Epsilon 0.39299999999999946 mean q 15.599246\n",
      "Episode:  607 Reward: 33.0 Epsilon 0.39199999999999946 mean q 15.266603\n",
      "Episode:  608 Reward: 28.0 Epsilon 0.39099999999999946 mean q 14.855085\n",
      "Episode:  609 Reward: 70.0 Epsilon 0.38999999999999946 mean q 16.140692\n",
      "Episode:  610 Reward: 46.0 Epsilon 0.38899999999999946 mean q 15.187059\n",
      "Episode:  611 Reward: 60.0 Epsilon 0.38799999999999946 mean q 15.786922\n",
      "Episode:  612 Reward: 35.0 Epsilon 0.38699999999999946 mean q 15.157134\n",
      "Episode:  613 Reward: 43.0 Epsilon 0.38599999999999945 mean q 14.820683\n",
      "Episode:  614 Reward: 36.0 Epsilon 0.38499999999999945 mean q 14.847197\n",
      "Episode:  615 Reward: 68.0 Epsilon 0.38399999999999945 mean q 15.188625\n",
      "Episode:  616 Reward: 67.0 Epsilon 0.38299999999999945 mean q 15.22645\n",
      "Episode:  617 Reward: 39.0 Epsilon 0.38199999999999945 mean q 14.827983\n",
      "Episode:  618 Reward: 14.0 Epsilon 0.38099999999999945 mean q 12.673404\n",
      "Episode:  619 Reward: 115.0 Epsilon 0.37999999999999945 mean q 15.221401\n",
      "Episode:  620 Reward: 45.0 Epsilon 0.37899999999999945 mean q 15.302217\n",
      "Episode:  621 Reward: 52.0 Epsilon 0.37799999999999945 mean q 15.196319\n",
      "Episode:  622 Reward: 44.0 Epsilon 0.37699999999999945 mean q 16.347685\n",
      "Episode:  623 Reward: 37.0 Epsilon 0.37599999999999945 mean q 15.18123\n",
      "Episode:  624 Reward: 63.0 Epsilon 0.37499999999999944 mean q 15.193466\n",
      "Episode:  625 Reward: 53.0 Epsilon 0.37399999999999944 mean q 15.341702\n",
      "Episode:  626 Reward: 86.0 Epsilon 0.37299999999999944 mean q 16.183756\n",
      "Episode:  627 Reward: 51.0 Epsilon 0.37199999999999944 mean q 15.337543\n",
      "Episode:  628 Reward: 69.0 Epsilon 0.37099999999999944 mean q 15.216981\n",
      "Episode:  629 Reward: 32.0 Epsilon 0.36999999999999944 mean q 14.813233\n",
      "Episode:  630 Reward: 30.0 Epsilon 0.36899999999999944 mean q 16.088964\n",
      "Episode:  631 Reward: 57.0 Epsilon 0.36799999999999944 mean q 15.091668\n",
      "Episode:  632 Reward: 47.0 Epsilon 0.36699999999999944 mean q 14.7601\n",
      "Episode:  633 Reward: 41.0 Epsilon 0.36599999999999944 mean q 16.32794\n",
      "Episode:  634 Reward: 86.0 Epsilon 0.36499999999999944 mean q 15.400391\n",
      "Episode:  635 Reward: 78.0 Epsilon 0.36399999999999944 mean q 15.1307745\n",
      "Episode:  636 Reward: 32.0 Epsilon 0.36299999999999943 mean q 14.658215\n",
      "Episode:  637 Reward: 16.0 Epsilon 0.36199999999999943 mean q 12.849212\n",
      "Episode:  638 Reward: 65.0 Epsilon 0.36099999999999943 mean q 15.089125\n",
      "Episode:  639 Reward: 39.0 Epsilon 0.35999999999999943 mean q 15.178069\n",
      "Episode:  640 Reward: 55.0 Epsilon 0.35899999999999943 mean q 15.312037\n",
      "Episode:  641 Reward: 28.0 Epsilon 0.35799999999999943 mean q 14.928952\n",
      "Episode:  642 Reward: 55.0 Epsilon 0.35699999999999943 mean q 14.6401615\n",
      "Episode:  643 Reward: 44.0 Epsilon 0.35599999999999943 mean q 14.832856\n",
      "Episode:  644 Reward: 51.0 Epsilon 0.3549999999999994 mean q 14.690518\n",
      "Episode:  645 Reward: 74.0 Epsilon 0.3539999999999994 mean q 14.753517\n",
      "Episode:  646 Reward: 84.0 Epsilon 0.3529999999999994 mean q 15.066127\n",
      "Episode:  647 Reward: 50.0 Epsilon 0.3519999999999994 mean q 14.905315\n",
      "Episode:  648 Reward: 52.0 Epsilon 0.3509999999999994 mean q 14.965048\n",
      "Episode:  649 Reward: 46.0 Epsilon 0.3499999999999994 mean q 14.913866\n",
      "Episode:  650 Reward: 68.0 Epsilon 0.3489999999999994 mean q 15.082908\n",
      "Episode:  651 Reward: 52.0 Epsilon 0.3479999999999994 mean q 15.012402\n",
      "Episode:  652 Reward: 45.0 Epsilon 0.3469999999999994 mean q 14.944226\n",
      "Episode:  653 Reward: 54.0 Epsilon 0.3459999999999994 mean q 16.829004\n",
      "Episode:  654 Reward: 52.0 Epsilon 0.3449999999999994 mean q 14.932097\n",
      "Episode:  655 Reward: 53.0 Epsilon 0.3439999999999994 mean q 16.596395\n",
      "Episode:  656 Reward: 70.0 Epsilon 0.3429999999999994 mean q 15.0343275\n",
      "Episode:  657 Reward: 55.0 Epsilon 0.3419999999999994 mean q 14.850541\n",
      "Episode:  658 Reward: 38.0 Epsilon 0.3409999999999994 mean q 14.620506\n",
      "Episode:  659 Reward: 200.0 Epsilon 0.3399999999999994 mean q 15.34606\n",
      "Episode:  660 Reward: 61.0 Epsilon 0.3389999999999994 mean q 14.953792\n",
      "Episode:  661 Reward: 68.0 Epsilon 0.3379999999999994 mean q 14.86232\n",
      "Episode:  662 Reward: 142.0 Epsilon 0.3369999999999994 mean q 15.254101\n",
      "Episode:  663 Reward: 32.0 Epsilon 0.3359999999999994 mean q 14.466683\n",
      "Episode:  664 Reward: 53.0 Epsilon 0.3349999999999994 mean q 14.8052635\n",
      "Episode:  665 Reward: 34.0 Epsilon 0.3339999999999994 mean q 14.539024\n",
      "Episode:  666 Reward: 80.0 Epsilon 0.3329999999999994 mean q 15.493376\n",
      "Episode:  667 Reward: 58.0 Epsilon 0.3319999999999994 mean q 14.697674\n",
      "Episode:  668 Reward: 66.0 Epsilon 0.3309999999999994 mean q 14.609219\n",
      "Episode:  669 Reward: 166.0 Epsilon 0.3299999999999994 mean q 15.393961\n",
      "Episode:  670 Reward: 113.0 Epsilon 0.3289999999999994 mean q 15.250553\n",
      "Episode:  671 Reward: 56.0 Epsilon 0.3279999999999994 mean q 14.602766\n",
      "Episode:  672 Reward: 66.0 Epsilon 0.3269999999999994 mean q 14.48043\n",
      "Episode:  673 Reward: 20.0 Epsilon 0.3259999999999994 mean q 13.922818\n",
      "Episode:  674 Reward: 54.0 Epsilon 0.3249999999999994 mean q 14.322517\n",
      "Episode:  675 Reward: 47.0 Epsilon 0.3239999999999994 mean q 14.586598\n",
      "Episode:  676 Reward: 44.0 Epsilon 0.3229999999999994 mean q 14.09946\n",
      "Episode:  677 Reward: 72.0 Epsilon 0.3219999999999994 mean q 14.723415\n",
      "Episode:  678 Reward: 142.0 Epsilon 0.3209999999999994 mean q 16.088663\n",
      "Episode:  679 Reward: 42.0 Epsilon 0.3199999999999994 mean q 14.413954\n",
      "Episode:  680 Reward: 40.0 Epsilon 0.3189999999999994 mean q 14.17097\n",
      "Episode:  681 Reward: 78.0 Epsilon 0.3179999999999994 mean q 14.830757\n",
      "Episode:  682 Reward: 63.0 Epsilon 0.3169999999999994 mean q 17.47807\n",
      "Episode:  683 Reward: 71.0 Epsilon 0.3159999999999994 mean q 14.889657\n",
      "Episode:  684 Reward: 194.0 Epsilon 0.3149999999999994 mean q 15.973258\n",
      "Episode:  685 Reward: 77.0 Epsilon 0.3139999999999994 mean q 16.738632\n",
      "Episode:  686 Reward: 53.0 Epsilon 0.3129999999999994 mean q 14.1840925\n",
      "Episode:  687 Reward: 62.0 Epsilon 0.3119999999999994 mean q 14.435726\n",
      "Episode:  688 Reward: 59.0 Epsilon 0.3109999999999994 mean q 14.217225\n",
      "Episode:  689 Reward: 122.0 Epsilon 0.3099999999999994 mean q 16.449125\n",
      "Episode:  690 Reward: 64.0 Epsilon 0.3089999999999994 mean q 13.973504\n",
      "Episode:  691 Reward: 85.0 Epsilon 0.3079999999999994 mean q 14.681698\n",
      "Episode:  692 Reward: 30.0 Epsilon 0.3069999999999994 mean q 14.854975\n",
      "Episode:  693 Reward: 61.0 Epsilon 0.3059999999999994 mean q 14.650711\n",
      "Episode:  694 Reward: 65.0 Epsilon 0.3049999999999994 mean q 14.082033\n",
      "Episode:  695 Reward: 67.0 Epsilon 0.3039999999999994 mean q 14.544592\n",
      "Episode:  696 Reward: 55.0 Epsilon 0.3029999999999994 mean q 13.98844\n",
      "Episode:  697 Reward: 138.0 Epsilon 0.3019999999999994 mean q 16.806122\n",
      "Episode:  698 Reward: 43.0 Epsilon 0.3009999999999994 mean q 14.084257\n",
      "Episode:  699 Reward: 64.0 Epsilon 0.2999999999999994 mean q 14.112001\n",
      "Episode:  700 Reward: 124.0 Epsilon 0.2989999999999994 mean q 14.866875\n",
      "Episode:  701 Reward: 74.0 Epsilon 0.2979999999999994 mean q 13.941655\n",
      "Episode:  702 Reward: 147.0 Epsilon 0.2969999999999994 mean q 14.927714\n",
      "Episode:  703 Reward: 154.0 Epsilon 0.2959999999999994 mean q 16.754818\n",
      "Episode:  704 Reward: 95.0 Epsilon 0.2949999999999994 mean q 14.197557\n",
      "Episode:  705 Reward: 147.0 Epsilon 0.2939999999999994 mean q 16.526258\n",
      "Episode:  706 Reward: 63.0 Epsilon 0.29299999999999937 mean q 17.534357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  707 Reward: 94.0 Epsilon 0.29199999999999937 mean q 14.157935\n",
      "Episode:  708 Reward: 81.0 Epsilon 0.29099999999999937 mean q 16.934504\n",
      "Episode:  709 Reward: 38.0 Epsilon 0.28999999999999937 mean q 14.706531\n",
      "Episode:  710 Reward: 56.0 Epsilon 0.28899999999999937 mean q 17.117323\n",
      "Episode:  711 Reward: 89.0 Epsilon 0.28799999999999937 mean q 14.197084\n",
      "Episode:  712 Reward: 118.0 Epsilon 0.28699999999999937 mean q 14.678026\n",
      "Episode:  713 Reward: 83.0 Epsilon 0.28599999999999937 mean q 14.158306\n",
      "Episode:  714 Reward: 84.0 Epsilon 0.28499999999999936 mean q 14.275283\n",
      "Episode:  715 Reward: 101.0 Epsilon 0.28399999999999936 mean q 16.609219\n",
      "Episode:  716 Reward: 171.0 Epsilon 0.28299999999999936 mean q 15.513946\n",
      "Episode:  717 Reward: 145.0 Epsilon 0.28199999999999936 mean q 16.623795\n",
      "Episode:  718 Reward: 161.0 Epsilon 0.28099999999999936 mean q 15.02739\n",
      "Episode:  719 Reward: 103.0 Epsilon 0.27999999999999936 mean q 17.185509\n",
      "Episode:  720 Reward: 76.0 Epsilon 0.27899999999999936 mean q 13.911568\n",
      "Episode:  721 Reward: 103.0 Epsilon 0.27799999999999936 mean q 14.485581\n",
      "Episode:  722 Reward: 104.0 Epsilon 0.27699999999999936 mean q 14.334132\n",
      "Episode:  723 Reward: 56.0 Epsilon 0.27599999999999936 mean q 17.433825\n",
      "Episode:  724 Reward: 151.0 Epsilon 0.27499999999999936 mean q 15.3574295\n",
      "Episode:  725 Reward: 115.0 Epsilon 0.27399999999999936 mean q 17.075935\n",
      "Episode:  726 Reward: 128.0 Epsilon 0.27299999999999935 mean q 14.664891\n",
      "Episode:  727 Reward: 56.0 Epsilon 0.27199999999999935 mean q 14.083052\n",
      "Episode:  728 Reward: 128.0 Epsilon 0.27099999999999935 mean q 16.437468\n",
      "Episode:  729 Reward: 102.0 Epsilon 0.26999999999999935 mean q 13.752854\n",
      "Episode:  730 Reward: 150.0 Epsilon 0.26899999999999935 mean q 14.986842\n",
      "Episode:  731 Reward: 72.0 Epsilon 0.26799999999999935 mean q 17.216606\n",
      "Episode:  732 Reward: 172.0 Epsilon 0.26699999999999935 mean q 16.705984\n",
      "Episode:  733 Reward: 58.0 Epsilon 0.26599999999999935 mean q 14.601813\n",
      "Episode:  734 Reward: 121.0 Epsilon 0.26499999999999935 mean q 15.245502\n",
      "Episode:  735 Reward: 104.0 Epsilon 0.26399999999999935 mean q 14.318815\n",
      "Episode:  736 Reward: 157.0 Epsilon 0.26299999999999935 mean q 16.402626\n",
      "Episode:  737 Reward: 174.0 Epsilon 0.26199999999999934 mean q 16.269989\n",
      "Episode:  738 Reward: 137.0 Epsilon 0.26099999999999934 mean q 14.278809\n",
      "Episode:  739 Reward: 80.0 Epsilon 0.25999999999999934 mean q 13.627144\n",
      "Episode:  740 Reward: 105.0 Epsilon 0.25899999999999934 mean q 14.035379\n",
      "Episode:  741 Reward: 102.0 Epsilon 0.25799999999999934 mean q 16.80523\n",
      "Episode:  742 Reward: 132.0 Epsilon 0.25699999999999934 mean q 16.73143\n",
      "Episode:  743 Reward: 104.0 Epsilon 0.25599999999999934 mean q 13.6644335\n",
      "Episode:  744 Reward: 90.0 Epsilon 0.25499999999999934 mean q 13.711207\n",
      "Episode:  745 Reward: 156.0 Epsilon 0.25399999999999934 mean q 14.98489\n",
      "Episode:  746 Reward: 200.0 Epsilon 0.25299999999999934 mean q 16.855639\n",
      "Episode:  747 Reward: 89.0 Epsilon 0.25199999999999934 mean q 13.93104\n",
      "Episode:  748 Reward: 86.0 Epsilon 0.25099999999999933 mean q 14.045146\n",
      "Episode:  749 Reward: 155.0 Epsilon 0.24999999999999933 mean q 15.194027\n",
      "Episode:  750 Reward: 139.0 Epsilon 0.24899999999999933 mean q 14.495982\n",
      "Episode:  751 Reward: 137.0 Epsilon 0.24799999999999933 mean q 15.803836\n",
      "Episode:  752 Reward: 164.0 Epsilon 0.24699999999999933 mean q 14.885126\n",
      "Episode:  753 Reward: 200.0 Epsilon 0.24599999999999933 mean q 16.333723\n",
      "Episode:  754 Reward: 194.0 Epsilon 0.24499999999999933 mean q 15.694036\n",
      "Episode:  755 Reward: 154.0 Epsilon 0.24399999999999933 mean q 16.023561\n",
      "Episode:  756 Reward: 116.0 Epsilon 0.24299999999999933 mean q 15.12239\n",
      "Episode:  757 Reward: 200.0 Epsilon 0.24199999999999933 mean q 16.537903\n",
      "Episode:  758 Reward: 101.0 Epsilon 0.24099999999999933 mean q 14.471409\n",
      "Episode:  759 Reward: 200.0 Epsilon 0.23999999999999932 mean q 16.61207\n",
      "Episode:  760 Reward: 109.0 Epsilon 0.23899999999999932 mean q 13.301787\n",
      "Episode:  761 Reward: 200.0 Epsilon 0.23799999999999932 mean q 16.80237\n",
      "Episode:  762 Reward: 142.0 Epsilon 0.23699999999999932 mean q 14.953699\n",
      "Episode:  763 Reward: 167.0 Epsilon 0.23599999999999932 mean q 15.029424\n",
      "Episode:  764 Reward: 113.0 Epsilon 0.23499999999999932 mean q 15.367808\n",
      "Episode:  765 Reward: 200.0 Epsilon 0.23399999999999932 mean q 15.873295\n",
      "Episode:  766 Reward: 196.0 Epsilon 0.23299999999999932 mean q 15.991248\n",
      "Episode:  767 Reward: 134.0 Epsilon 0.23199999999999932 mean q 14.905852\n",
      "Episode:  768 Reward: 158.0 Epsilon 0.23099999999999932 mean q 14.914742\n",
      "Episode:  769 Reward: 173.0 Epsilon 0.22999999999999932 mean q 15.463365\n",
      "Episode:  770 Reward: 127.0 Epsilon 0.22899999999999932 mean q 15.501784\n",
      "Episode:  771 Reward: 194.0 Epsilon 0.22799999999999931 mean q 15.471714\n",
      "Episode:  772 Reward: 117.0 Epsilon 0.2269999999999993 mean q 14.529684\n",
      "Episode:  773 Reward: 200.0 Epsilon 0.2259999999999993 mean q 17.650955\n",
      "Episode:  774 Reward: 200.0 Epsilon 0.2249999999999993 mean q 15.909621\n",
      "Episode:  775 Reward: 175.0 Epsilon 0.2239999999999993 mean q 15.944812\n",
      "Episode:  776 Reward: 186.0 Epsilon 0.2229999999999993 mean q 15.501964\n",
      "Episode:  777 Reward: 200.0 Epsilon 0.2219999999999993 mean q 17.716427\n",
      "Episode:  778 Reward: 130.0 Epsilon 0.2209999999999993 mean q 14.991529\n",
      "Episode:  779 Reward: 200.0 Epsilon 0.2199999999999993 mean q 15.601007\n",
      "Episode:  780 Reward: 200.0 Epsilon 0.2189999999999993 mean q 15.935146\n",
      "Episode:  781 Reward: 200.0 Epsilon 0.2179999999999993 mean q 17.109203\n",
      "Episode:  782 Reward: 146.0 Epsilon 0.2169999999999993 mean q 15.461488\n",
      "Episode:  783 Reward: 189.0 Epsilon 0.2159999999999993 mean q 15.761621\n",
      "Episode:  784 Reward: 200.0 Epsilon 0.2149999999999993 mean q 17.4417\n",
      "Episode:  785 Reward: 200.0 Epsilon 0.2139999999999993 mean q 16.087784\n",
      "Episode:  786 Reward: 200.0 Epsilon 0.2129999999999993 mean q 16.67545\n",
      "Episode:  787 Reward: 200.0 Epsilon 0.2119999999999993 mean q 17.806114\n",
      "Episode:  788 Reward: 200.0 Epsilon 0.2109999999999993 mean q 17.294548\n",
      "Episode:  789 Reward: 200.0 Epsilon 0.2099999999999993 mean q 17.495705\n",
      "Episode:  790 Reward: 200.0 Epsilon 0.2089999999999993 mean q 18.110315\n",
      "Episode:  791 Reward: 200.0 Epsilon 0.2079999999999993 mean q 16.55671\n",
      "Episode:  792 Reward: 172.0 Epsilon 0.2069999999999993 mean q 14.162455\n",
      "Episode:  793 Reward: 200.0 Epsilon 0.2059999999999993 mean q 17.958057\n",
      "Episode:  794 Reward: 146.0 Epsilon 0.2049999999999993 mean q 14.743499\n",
      "Episode:  795 Reward: 200.0 Epsilon 0.2039999999999993 mean q 14.760811\n",
      "Episode:  796 Reward: 192.0 Epsilon 0.2029999999999993 mean q 15.76826\n",
      "Episode:  797 Reward: 168.0 Epsilon 0.2019999999999993 mean q 15.417749\n",
      "Episode:  798 Reward: 200.0 Epsilon 0.2009999999999993 mean q 17.768982\n",
      "Episode:  799 Reward: 200.0 Epsilon 0.1999999999999993 mean q 15.72575\n",
      "Episode:  800 Reward: 200.0 Epsilon 0.1989999999999993 mean q 17.1961\n",
      "Episode:  801 Reward: 200.0 Epsilon 0.1979999999999993 mean q 15.522478\n",
      "Episode:  802 Reward: 200.0 Epsilon 0.1969999999999993 mean q 18.326443\n",
      "Episode:  803 Reward: 200.0 Epsilon 0.19599999999999929 mean q 17.581549\n",
      "Episode:  804 Reward: 200.0 Epsilon 0.19499999999999929 mean q 18.681913\n",
      "Episode:  805 Reward: 200.0 Epsilon 0.19399999999999928 mean q 18.704826\n",
      "Episode:  806 Reward: 197.0 Epsilon 0.19299999999999928 mean q 16.104383\n",
      "Episode:  807 Reward: 200.0 Epsilon 0.19199999999999928 mean q 18.095318\n",
      "Episode:  808 Reward: 198.0 Epsilon 0.19099999999999928 mean q 16.15131\n",
      "Episode:  809 Reward: 200.0 Epsilon 0.18999999999999928 mean q 18.354729\n",
      "Episode:  810 Reward: 200.0 Epsilon 0.18899999999999928 mean q 18.005173\n",
      "Episode:  811 Reward: 200.0 Epsilon 0.18799999999999928 mean q 18.487755\n",
      "Episode:  812 Reward: 200.0 Epsilon 0.18699999999999928 mean q 18.977211\n",
      "Episode:  813 Reward: 200.0 Epsilon 0.18599999999999928 mean q 17.453564\n",
      "Episode:  814 Reward: 200.0 Epsilon 0.18499999999999928 mean q 17.755848\n",
      "Episode:  815 Reward: 200.0 Epsilon 0.18399999999999928 mean q 18.396093\n",
      "Episode:  816 Reward: 200.0 Epsilon 0.18299999999999927 mean q 18.411469\n",
      "Episode:  817 Reward: 200.0 Epsilon 0.18199999999999927 mean q 18.37724\n",
      "Episode:  818 Reward: 190.0 Epsilon 0.18099999999999927 mean q 16.637638\n",
      "Episode:  819 Reward: 200.0 Epsilon 0.17999999999999927 mean q 18.77298\n",
      "Episode:  820 Reward: 200.0 Epsilon 0.17899999999999927 mean q 18.360952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  821 Reward: 200.0 Epsilon 0.17799999999999927 mean q 18.619541\n",
      "Episode:  822 Reward: 191.0 Epsilon 0.17699999999999927 mean q 16.887918\n",
      "Episode:  823 Reward: 200.0 Epsilon 0.17599999999999927 mean q 18.097305\n",
      "Episode:  824 Reward: 200.0 Epsilon 0.17499999999999927 mean q 18.837233\n",
      "Episode:  825 Reward: 200.0 Epsilon 0.17399999999999927 mean q 18.846642\n",
      "Episode:  826 Reward: 200.0 Epsilon 0.17299999999999927 mean q 17.114214\n",
      "Episode:  827 Reward: 200.0 Epsilon 0.17199999999999926 mean q 16.7298\n",
      "Episode:  828 Reward: 200.0 Epsilon 0.17099999999999926 mean q 18.9058\n",
      "Episode:  829 Reward: 200.0 Epsilon 0.16999999999999926 mean q 18.219223\n",
      "Episode:  830 Reward: 200.0 Epsilon 0.16899999999999926 mean q 17.633234\n",
      "Episode:  831 Reward: 200.0 Epsilon 0.16799999999999926 mean q 18.640774\n"
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Initializations\n",
    "num_actions = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Our Neural Netork model used to estimate the Q-values\n",
    "model = DoubleQLearningModel(state_dim=obs_dim, action_dim=num_actions, learning_rate=1e-4)\n",
    "\n",
    "# Create replay buffer, where experience in form of tuples <s,a,r,s',t>, gathered from the environment is stored \n",
    "# for training\n",
    "replay_buffer = ExperienceReplay(state_size=obs_dim)\n",
    "\n",
    "# Train\n",
    "num_episodes = 1200 \n",
    "batch_size = 128 \n",
    "R, R_avg = train_loop_ddqn(model, env, num_episodes, batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close window\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the code above, and the code in the provided .py file, answer the following questions:\n",
    "    \n",
    "What is the state for this problem?\n",
    "\n",
    "**Your answer**: (fill in here)\n",
    "\n",
    "When do we switch the networks (i.e. when does the online network become the fixed one, and vice-versa)?\n",
    "\n",
    "**Your answer**: (fill in here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to visualize your final policy in an episode from this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "num_episodes = 1\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "for i in range(num_episodes):\n",
    "        state = env.reset() #reset to initial state\n",
    "        state = np.expand_dims(state, axis=0)/2\n",
    "        terminal = False # reset terminal flag\n",
    "        while not terminal:\n",
    "            env.render()\n",
    "            time.sleep(.05)\n",
    "            q_values = model.get_q_values(state)\n",
    "            policy = eps_greedy_policy(q_values.squeeze(), .1) # greedy policy\n",
    "            action = np.random.choice(num_actions, p=policy)\n",
    "            state, reward, terminal, _ = env.step(action) # take one step in the evironment\n",
    "            state = np.expand_dims(state, axis=0)/2\n",
    "# close window\n",
    "env.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the episodic rewards obtained throughout the optimization, together with a moving average of it (since the episodic reward is usually very noisy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rewards = plt.plot(R, alpha=.4, label='R')\n",
    "avg_rewards = plt.plot(R_avg,label='avg R')\n",
    "plt.legend(bbox_to_anchor=(1.01, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylim(0, 210)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have now successfully implemented the DDQN algorithm. You are encouraged to explore different problems. There are a lot of different environments ready for you to implement your algorithms in. A few of these resources are:\n",
    "* [OpenAI gym](https://github.com/openai/gym)\n",
    "* [OpenAI Universe](https://github.com/openai/universe)\n",
    "* [DeepMind Lab](https://deepmind.com/blog/open-sourcing-deepmind-lab/)\n",
    "\n",
    "The model you implemented in this lab can be extended to solve harder problems. A good starting-point is to try to solve the Acrobot-problem, by loading the environment as \n",
    "\n",
    "**gym.make(\"Acrobot-v1\")**.\n",
    "\n",
    "The problem might require some modifications to how you decay $\\epsilon$, but otherwise, the code you have written within this lab should be sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2 Atari games\n",
    "\n",
    "A common benchmark for reinforcement learning algorithms is the old Atari games. For the Atari games, each observation consists of one screenshot of the current state of the game. Other than adding convolutional layers to your neural network, there is one more issue regarding the new input that needs to be solved. Name at least two solutions to the problem, and why it won't work without these changes. \n",
    "\n",
    "Hint:\n",
    "- Imagine the game of pong. What is important for the algorithm to predict? What is the input to the algorithm? Is it possible to predict what we want from the input given?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer:** (fill in here)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
